# Significance Tests and Power {#day4}

## The Central Limit Theorem (CLT)

From distributions of measurements
to distributions of means.

What if Johanna didn't count as many cells?
Say 30 instead of 300?

To turn this into a demonstration, we assume
that the 300 cells are completely representative
of the all WT and KO cells and thus call this
our population.

- Now we draw a sample of 30 from the population
- We calculate the mean
- And repeat this procedure 1000 times
- What does the distribution of means look like?

### Reading in Data

Firstly, we load the tidyverse and our data.

```{r}
library(tidyverse)
```

```{r}
# import data for inclusion bodies

# attach data, handle with CARE!
```

`attach` makes the columns of a datensets (in our case _ko_ and _wt_)
globally available as vectors. This enables us to write
`wt` intead `data$wt` or `data["wt"]`.

> Use `attach` carefully and only in very few situations!
  The order of those two vectors is now independent of each
  other. Operations on the vectors `wt` und `ko` do not
  influence the other vector or the original column
  of the dataframe `data`!
  
### Programming and Problemsolving

#### The basic Case

First, we solve the first part of our roadmap:

- Now we draw a sample of 30 from the population
- We calculate the mean

```{r}
# sample n = 30 points from a vector

# calculate mean
```

#### Abstraction for many Cases

We do not want to type the above lines 1000 times.
One way to solve this would be with a `for`-loop.
But there is a more elegant way.
This might seem daunting at first but will
enable you to solve very complex problems
faster in the long run.

We turn the two lines of code above into a function.
And while doing so also introduce a bit of abstraction
(generalization)

We create a function that takes a vector (`x`),
draws `n` elements from it and returns the mean of those elements.

```{r}
# getMeanOfSubset
```

Now we can now test that the function is working.

```{r}
# test getMeanOfSubset
```

We now use the `map` function from the `purrr`-package
to call this function 1000 times and save the result
in a list or vector (`map_dbl` for map _double_).

```{r}
# define N
# map over 1:N
```

Now you might be asking yourselves: "What is this weird tild
symbol doing there?"

#### Excursion: Lambda Functions

$$\lambda$$

The `~` symbol (tilde) in the context
of `map` functions creates a so called _lambda_ function.
_Lambda_ functions are anonymous functions, functions
without a name. So we generally use them whenever
we need a function only once.


`add_one <- function(x) x + 1`
is equivalent to
`~ .x + 1`.

The parameter passed to the lambda function is
always called `.x` inside of the function.

So the lambda funcion `~ getMeanOfSubset(wt, 30)`
takes the numbers from 1 to N (`1:N`) but completely
ignores them (there is no mention of `.x`). 
The result is thus `getMeanOfSubset` beeing called
1000 times with the same arguments (`wt` and `30`).

```{r}
# show replicate as the special case for functions
# that don't use the elements of the vector

```

### Population Distribution vs. Distribution of Means

With large enough n, the distribtion of means
follows a normal distribution, even though
the values are not normally distributed.

```{r}
# hist of Ms
```

```{r}
# hist of wt
```

According to the Central Limit Theorem the means
follow a normal distribution. We can show this.

```{r}
# show qqnorm of Ms
# and qqline
```

## The T-Distribution

The CLT is only valid for large  _sample sizes_.
For smaller sample sizes, the distribution of
means has fatter tails than a normal distribution

This is why for most statistical tests,
we use the t-distribtion instead of the
normal distribution.

For 3 degrees of freedom (DF):

```{r tdist, fig.cap="t-distribution in red, normal distribution in black."}
tdist <- function(x) dt(x, df = 3)
curve(dnorm, -3, 3)
curve(tdist, -3, 3, add = TRUE, col = "red")
```

For 30 DF:

```{r tdist2, fig.cap="t-distribution in red, normal distribution in black."}
tdist <- function(x) dt(x, df = 30)
curve(dnorm, -3, 3)
curve(tdist, -3, 3, add = TRUE, col = "red")
```

### Confidence Intervalls (CIs)

We can use the t-distribution to
calculate (95%-) confidence intervals.

The 95% CI of a sample mean contains the
true mean (the mean of the population)
in 95% of cases (if you where to repeat the experiment
infenitely often).

```{r ci-vis, fig.cap="True mean in red."}
# show hist of Ms

# add line for true mean of wt, lwd = 3

```

```{r}
# use t.test for conf.int of a sample (30) from wt

# get limits with $conf.int

# hist of sample

# add lines with CI
```

## Significance Tests

Significance tests answer the question:

> _"Under the assumption of no difference
  between two (or more) groups (i.e. they
  are samples of the same population),
  what is the probability to find a difference
  as large as or more extreme than the
  difference observed."_

### Students T-Test

For normally distributed data,
we would use students t-test.

```{r, eval=FALSE}
t.test(wt, ko)
```


```{r, echo = FALSE}
read_csv("data/03_inclusion_bodies.csv") %>% 
  pivot_longer(1:2) %>% 
  ggplot(aes(value, fill = name)) +
  geom_histogram(position = "identity") +
  theme(legend.position = "bottom")
```

```{r}

```

```{r}
subset_wt <- sample(wt, 3)
subset_ko <- sample(ko, 3)
t.test(subset_wt, subset_ko)
```

### Wilcoxon Rank Sum Test

Sind unsere Daten nicht normalverteilt,
wie es hier der Fall ist, ist ein sogenannter
nicht-parametrischer Test angebracht. Diese Tests
machen nicht die Annahme der Normalität.

Der Wilcoxon Rank Sum Test (Auch Mann-Whitney U Test genannt)
wandelt die eigentlichen Werte der Datenpunkte zunächst
in Ränge um. Also:

> "Der wievieltniedrigste Punkt ist es?"

Beispiel:

```{r}
x <- c(1, 2, 20, 1239132, 3, 5)
rank(x)
```

Und an unseren Daten:

```{r, echo = FALSE}
data %>%
  pivot_longer(c(1,2)) %>% 
  ggplot(aes(name, value)) +
  geom_jitter() +
  labs(x = "")
```

```{r}
wilcox.test(wt, ko)
```

## Type I und Type II Errors

- Type I:  False Positives (rejection of a true null hypothesis)
- Type II: False Negatives (non-rejection of false null hypothesis)

### Type I: False Positives

Ein Typ I Fehler würde bedeuten, dass wir sagen, ein Unterschied
zwischen den Gruppen existiere,
obwohl alle Werte aus der gleichen Verteilung stammen und daher kein
Unterschied besteht.

Bei einem P-Value von $\leq$ 0.05 (= 5\ %) wird typischerweise
gesagt, dass ein "statistisch signifikanter" Unterschied besteht.
Mit diesem typischen Cutoff von 0.05 akzeptieren
wir also, allein durch unsere Definition, bereits (mindestens)
5\ % falsch Positive.

Dieser Cutoff, das Signifikanzlevel, wird auch $\alpha$ genannt.

$$\alpha=\text{Type I error rate}$$

```{r}
curve(dnorm, -3 , 3)
```

```{r}
# define n
n <- 3

# draw twice from the same normal distributions
draw1 <- rnorm(n)
draw2 <- rnorm(n)

# do the t.test
t.test(draw1, draw2)

# do the rank sum test
wilcox.test(draw1,  draw2)
```
 
Teste diesen Code mit unterschiedlichen Werten für `n`.

### Type II errors

Wenn zwischen zwei Gruppen ein tatsächlicher Unterschied besteht,
wir aber bei unserem Test ein nicht signifikantes Ergebnis erhalten,
begehen wir einen Fehler vom Typ II.

Ein Typ II Fehler ist also ein falsch negatives Ergebnis.

$$\beta=\text{Type II error rate}$$

```{r}
# define n
n <- 15

# draw from different normal distributions
draw1 <- rnorm(n, 0, 1)
draw2 <- rnorm(n, 1, 1)

# convert to tibble, pivot_longer, plot points
tibble(draw1, draw2) %>% 
  pivot_longer(c(1,2)) %>% 
  ggplot(aes(name, value, group = 1)) +
  geom_jitter(width = 0.2) +
  stat_summary(geom = "line", fun.y = mean, lwd = 1, lty = 2) +
  stat_summary(geom = "point", fun.y = mean, color = "red", size = 2)

# do the t.test
t.test(draw1, draw2)

# do the rank sum test
wilcox.test(draw1,  draw2)
```


### Statistical Power

Als statistische Power bezeichnen wir die Wahrscheinlichkeit eines Tests,
einen wahren Unterschied zwischen Gruppen bei dem gewählten $\alpha$ auch
tatsächlich als statistisch signifikant zu kennzeichnen
und damit ein wahr positives Ergebnis zu produzieren.

$$Power = 1-\beta$$

Für den T-Test können wir in R die folgende Funktion verwenden, die uns hier beispielsweise
die Frage beantwortet: "Wie viele Proben muss ich pro Gruppe (mindestens) nehmen,
um einen erwarteten Unterschied der Mittelwerte von 1 mit einer Standardabweichung von 1
in 80% der Fälle auch tatsächlich als solchen zu erkennen?"

```{r}
# show power.t.test
power.t.test(delta = 1, sd = 1, power = 0.8)
```

## Probleme

### The Jelly Bean Problem (Multiple Testing)

```{r beans, fig.cap="(Quelle: Randall Munroe, https://xkcd.com/882/ )", include=FALSE, eval=TRUE}
knitr::include_graphics("img/significant.png")
```

Die False Discovery Rate (FDA) kontrollieren:

- Bonferroni Korrektur

```{r}
# show p.adjust
p.adjust(c(0.05, 0.0001, 0.003323, 0.7), method = "bonferroni")
```

Jeder P-Value wird mutipliziert mit der Zahl der Tests

- Benjamini-Hochberg Prozedur
  - Ordne alle p-values in aufsteigender Reihenfolge
  - Wähle eine FDR ($q$) und nenne die Anzahl deiner Tests $m$
  - Finde den größten p-value für den gilt:
    $p \leq iq/m$ mit dem Index des p-values $i$.

```{r}
# show p.adjust for BH
p.adjust(c(0.05, 0.0001, 0.003323, 0.7), method = "BH")
```

### The Base Rate Fallacy

Beispiel: Mammogramm

- Sensitivity = Power =  true positive rate
- Specificity = true negative rate = $1-\alpha$

```{r}
# calculate 
total <- 1000
positives <- 10
negatives <- total - positives
sensitivity <- 0.9
specificity <- 1 - 0.08
true_positives  <- sensitivity * positives
false_positives <- (1 - specificity) * negatives
p_positive <- true_positives / (true_positives + false_positives)
p_positive
```


```{r, echo = FALSE}
theme_set(theme_void())

data.frame(
  parts = c("positives", "negatives"),
  vals = c(positives, negatives)
) %>% 
  ggplot(aes(fill = parts, values = vals)) +
  waffle::geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("white", "red"))
```

```{r, echo = FALSE}
data.frame(
  parts = c("detected positives",
            "not detected positives (false negatives)",
            "negatives"),
  vals = c(true_positives, positives - true_positives, negatives)
) %>% 
ggplot(aes(fill = parts, values = vals)) +
  waffle::geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("darkred", "white", "red"))
```

```{r, echo = FALSE}
data.frame(
  parts = c("detected positives",
            "not detected positives (false negatives)",
            "false positives",
            "true negatives"),
  vals = c(true_positives,
           positives - true_positives,
           round(false_positives),
           negatives)
) %>% 
  ggplot(aes(fill = parts, values = vals)) +
  waffle::geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("darkred", "palevioletred1", "red", "white"))
```

### Resourcen 

- Statistics Done Wrong: https://www.statisticsdonewrong.com/
- Intuitive Biostatistics, auch in der Uni-Bib: https://katalog.ub.uni-heidelberg.de/cgi-bin/titel.cgi?katkey=68260114&sess=050a1316767b181982c9bce94283e9ae&query=Intuitive%20Biostatistics
- https://www.graphpad.com/guides/prism/8/statistics/index.htm

## Exercises

### Show the following Statements with Simulations in R

#### Increasing the sample size n reduces the standard error of the mean with the square root of n.

- Ziehe 10 Zellen aus dem wt (oder ko) Vektor
- Berechne den Mittelwert
- Wiederhole das Ganze 1000 mal
- Berechne die SD der 1000 Mittelwerte
- Ziehe nun 40 statt 10 Zellen und wiederhole die Schritte
- Schreibe eine Funktion, die n Zellen zieht, die Schritte ausführt
  und die SD ausgibt
- Füttere die Zahlen von 1 bis 100 in die Funktion
  (mit map_dbl) und plotte das Ergebnis

#### Ein 95% Konfidenzintervall eines Samples enthält den Populationsmittelwert mit einer Wahrscheinlichkeit von 95%.

- Ziehe 30 Zellen aus dem wt (oder ko) Vektor
- Berechne die Limits des CI (Confidence Interval)
- Schreibe eine Funktion, oben genanntes tut und
  führe sie 1000 mal aus.
- Schreibe eine Funktion, die testet, ob ein Set an Limits den
  wahren Mittelwert einschließt
- Wende sie auf die 1000 Sets der Limits an
- Wie oft (prozentual) erhältst du TRUE?

###  Veranschauliche die folgenden Konzepte anhand von Simulationen auf den vorliegenden Daten

#### Sensitivity: Wie groß ist die Wahrscheinlichkeit abhängig von der Sample Size n mit einem Wilcoxon Rank Sum Test tatsächlich einen p-value <= 0.05 zu erhalten, wenn ein Unterschied existiert?

- Ziehe 1000 mal ein Sample von 30 je aus wt und ko und teste
  auf Signifikanz
- Wie viele der 1000 Versuche sind statistisch signifikant?
- Wie verändert sich diese Zahl, wenn 10 statt 30 gezogen werden?


#### Specificity: Unter der Voraussetzung, dass **kein** Unterschied zwischen den Bedingungen vorliegt, wie groß ist die Wahrscheinlichkeit, dennoch ein statistisch signifikantes Ergebnis zu erhalten?

- Stell dir vor, alle Zellen seien wie wt-Zellen
- Ziehe zwei mal 30 aus den wt-Zellen und lasse einen
  Wilcoxon Rank Sum Test laufen
- Widerhole das das Prozedere 1000 mal
- Wie oft ist das Ergebnis statistisch signifikant?


#### Resource for p-value Histograms

http://varianceexplained.org/statistics/interpreting-pvalue-histogram/


```{r}
knitr::opts_chunk$set(include = FALSE, eval=FALSE)
```

#### Solutions


```{r}
library(tidyverse)

read_csv("data/03_inclusion_bodies.csv") %>% attach()
```

```{r}
draw <- sample(wt, 10)
mean(draw)
```

```{r}
get_sample_mean <- function(x, n) {
  draw <- sample(x, n, replace = TRUE)
  mean(draw)
}
```

```{r}
get_sample_mean(wt, 10)
```

```{r}
N <- 1000
many_means <- map_dbl(1:N, ~ get_sample_mean(wt, 10) )
sd(many_means)
```

```{r}
many_means <- map_dbl(1:N, ~get_sample_mean(wt, 40))
sd(many_means)
```

```{r}
# explain default arguments and scoping
get_sd_of_many_means <- function(n, x, N = 1000) {
  many_means <- map_dbl(1:N, ~get_sample_mean(x, n))
  sd(many_means)
}
```

```{r}
n_max <- 100
sds_by_n <- map_dbl(1:n_max, get_sd_of_many_means, x = wt)
```

```{r}
plot(x = 1:n_max,
     y = sds_by_n, type = "p")

curve(sd(wt)/sqrt(x), add = TRUE, col = "red")
```

```{r}
# talk about difference between sample sd and population sd
```


#### Lösung

```{r}
test_samle_wilcox <- function(n) {
  draw_wt <- sample(wt, n, replace = TRUE)
  draw_ko <- sample(ko, n, replace = TRUE)
  wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value
}
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(30))
```

```{r}
hist(many_p_values)
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(10))
```

```{r}
hist(many_p_values)
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(4))
```

```{r}
hist(many_p_values)
```

```{r}
alpha = 0.05
mean(many_p_values <= alpha)
```

```{r}
# talk about power, effect size, difference between wilcox and t-test
```


#### Lösung

```{r}
test_same_wilcox <- function(n) {
  draw_wt <- sample(wt, n, replace = TRUE)
  draw_ko <- sample(wt, n, replace = TRUE)
  wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value
}
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_same_wilcox(30))
```

```{r}
hist(many_p_values)
```

```{r}
alpha = 0.05
mean(many_p_values <= alpha)
```

```{r}
# this is what we would expect from p-values!
```



#### Solutions

```{r}
draw <- sample(wt, 30)
```

```{r}
test_results <- t.test(draw)
```

```{r}
test_results
```

```{r}
summary(test_results)
```

```{r}
str(test_results)
```

```{r}
test_results$conf.int
```

```{r}
get_sample_ci <- function(x, n) {
  draw <- sample(x, n, replace = TRUE)
  t.test(draw)$conf.int
}
```

```{r}
get_sample_ci(wt, 30)
```

```{r}
CIs <- map(1:1000, ~get_sample_ci(wt, 30))
```

```{r}
head(CIs)
```

```{r}
# explain list subsetting
CIs[1]
```

```{r}
CIs[[1]]
```

```{r}
CIs[[1]][1]
```


```{r}
test_ci <- function(limits, true_mean) {
  limits[1] < true_mean & limits[2] > true_mean
}
```


```{r}
# sidenote
# note difference between & and &&
c(TRUE, TRUE) &  c(FALSE, TRUE)
```

```{r}
c(TRUE, TRUE) && c(FALSE, TRUE)
```


```{r}
results <- map_lgl(CIs, test_ci, true_mean = mean(wt))
head(results)
```

```{r}
mean(results)
```

```{r}
# talk about organisation of code
# source("test.R")
```

#### Weitere Hinweise

```{r}
# check out
?map
# for the subtle differences between:
map(1:100, ~ .x + 1)
# and
add_one <- function(x) x + 1
map(1:100, add_one)
# and
map_dbl(1:100, add_one)

# especially the meaning of ~ (speak: lambda)
map(1:5, ~ print("hi"))

# more examples
map(1:10, paste, "hi")
is_even <- function(x) x %% 2 == 0
map_lgl(1:10, is_even)

# Note: Often, you don't need a map function
# Because many functions in R are vectorised by default:
x <- 1:10
y <- 1:10
# thus, just write
x + y
# instead of
map2_dbl(x,y, `+`)
```


