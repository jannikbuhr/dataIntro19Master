# Significance Tests and Power {#day4}

## The Central Limit Theorem (CLT)

From distributions of measurements
to distributions of means.

What if Johanna didn't count as many cells?
Say 30 instead of 300?

To turn this into a demonstration, we assume
that the 300 cells are completely representative
of the all WT and KO cells and thus call this
our population.

- Now we draw a sample of 30 from the population
- We calculate the mean
- And repeat this procedure 1000 times
- What does the distribution of means look like?

### Reading in Data

Firstly, we load the tidyverse and our data.

```{r}
library(tidyverse)
```

```{r}
# import data for inclusion bodies

# attach data, handle with CARE!
```

`attach` makes the columns of a datensets (in our case _ko_ and _wt_)
globally available as vectors. This enables us to write
`wt` intead `data$wt` or `data["wt"]`.

> Use `attach` carefully and only in very few situations!
  The order of those two vectors is now independent of each
  other. Operations on the vectors `wt` und `ko` do not
  influence the other vector or the original column
  of the dataframe `data`!
  
### Programming and Problemsolving

#### The basic Case

First, we solve the first part of our roadmap:

- Now we draw a sample of 30 from the population
- We calculate the mean

```{r}
# sample n = 30 points from a vector

# calculate mean
```

#### Abstraction for many Cases

We do not want to type the above lines 1000 times.
One way to solve this would be with a `for`-loop.
But there is a more elegant way.
This might seem daunting at first but will
enable you to solve very complex problems
faster in the long run.

We turn the two lines of code above into a function.
And while doing so also introduce a bit of abstraction
(generalization)

We create a function that takes a vector (`x`),
draws `n` elements from it and returns the mean of those elements.

```{r}
# getMeanOfSubset
```

Now we can now test that the function is working.

```{r}
# test getMeanOfSubset
```

We now use the `map` function from the `purrr`-package
to call this function 1000 times and save the result
in a list or vector (`map_dbl` for map _double_).

```{r}
# define N
# map over 1:N
```

Now you might be asking yourselves: "What is this weird tild
symbol doing there?"

#### Excursion: Lambda Functions

$$\lambda$$

The `~` symbol (tilde) in the context
of `map` functions creates a so called _lambda_ function.
_Lambda_ functions are anonymous functions, functions
without a name. So we generally use them whenever
we need a function only once.


`add_one <- function(x) x + 1`
is equivalent to
`~ .x + 1`.

The parameter passed to the lambda function is
always called `.x` inside of the function.

So the lambda funcion `~ getMeanOfSubset(wt, 30)`
takes the numbers from 1 to N (`1:N`) but completely
ignores them (there is no mention of `.x`). 
The result is `getMeanOfSubset` beeing called
1000 times with the same arguments (`wt` and `30`).

```{r}
# show replicate as the special case for functions
# that don't use the elements of the vector

```

### Population Distribution vs. Distribution of Means

With large enough n, the distribtion of means
follows a normal distribution, even though
the values are not normally distributed.

```{r}
# hist of Ms
```

```{r}
# hist of wt
```

According to the Central Limit Theorem the means
follow a normal distribution. We can show this.

```{r}
# show qqnorm of Ms
# and qqline
```

## The T-Distribution

The CLT is only valid for large  _sample sizes_.
For smaller sample sizes, the distribution of
means has fatter tails than a normal distribution

This is why for most statistical tests,
we use the t-distribtion instead of the
normal distribution.

For 3 degrees of freedom (DF):

```{r tdist, fig.cap="t-distribution in red, normal distribution in black."}
# define tdist as dt of x for df = 3
# curve dnorm from -3 to 3
# add tdist curve in red
```

For 30 DF:

```{r tdist2, fig.cap="t-distribution in red, normal distribution in black."}
# define tdist as dt of x for df = 30
# curve dnorm from -3 to 3
# add tdist curve in red
```

### Confidence Intervalls (CIs)

We can use the t-distribution to
calculate (95%-) confidence intervals.

The 95% CI of a sample mean contains the
true mean (the mean of the population)
in 95% of cases (if you where to repeat the experiment
infenitely often).

```{r ci-vis, fig.cap="True mean in red."}
# show hist of Ms

# add line for true mean of wt, lwd = 3

```

```{r}
# use t.test for conf.int of a sample (30) from wt

# get limits with $conf.int

# hist of sample

# add lines with CI
```

## Significance Tests

Significance tests answer the question:

> _"Under the assumption of no difference
  between two (or more) groups (i.e. they
  are samples of the same population),
  what is the probability to find a difference
  as large as or more extreme than the
  difference observed."_

### Students T-Test

For normally distributed data,
we would use students t-test.

```{r}
# show t.test on wt and ko
```

```{r}
# show t.test on samples of data
```

But our data is not normally distributed!

```{r}
# take inclusion bodies, pivot longer, plot histograms
```

### Wilcoxon Rank Sum Test

With non-normal data, we need a so called
non-parametric test. Thoses tests do not
make the assumption of normality.

The Wilcoxon Rank Sum test (also called Mann-Whitney U test)
takes the values and converts them into "ranks"
before comparing them.

The rank of a value in our dataset answers the question:

> "When sorting the values from lowest to highest,
  what is the index of that value?"

Example:

```{r}
# define x

# call rank on x

```

With our data

```{r}
# pivot_longer, plot jitter, demonstrate ranks
```

```{r}
# run wilcox.test on wt and ko
```

## Type I und Type II Errors

- Type I:  False Positives (rejection of a true null hypothesis)
- Type II: False Negatives (non-rejection of false null hypothesis)

### Type I: False Positives

A type I error means we find a difference between
samples even though there is none (they are from
the same population)

We often define a p-value $\leq$ 0.05 (= 5\ %) as
a "statistically significant" result. Keep in mind,
that this cutoff is arbitrary and has no physical
meaning. This cutoff also means that by definition
we are accepting a minimum of 5% false positives!
This cutoff, the significancelevel is also called
$\alpha$.

$$\alpha=\text{Type I error rate}$$

```{r}
# show normal distribution
```

```{r}
# define n
n <- 3

# draw twice from the same normal distributions

# do the t.test


# do the rank sum test

```
 
Test this code with different values of `n`.

### Type II: False Negatives

If there exists a true difference between samples
(i.e. they come from a different population), but
we do not detect that difference ("not statistically significant"),
we commit a type II error.

A type II error is thus a false negative result.
the type II error rate is called  $\beta$
(analogous to $\alpha$).

$$\beta=\text{Type II error rate}$$

```{r}
# define n

# draw from different normal distributions


# convert to tibble, pivot_longer, plot points, add stat_summary for means

# do the t.test

# do the rank sum test
```

Repeat with different values of `n`.

### Statistical Power

The power of a statistical test is the probability
to classify a real difference as statistically
significant (with the chosen $\alpha$).
So it represents true positives.

$$Power = 1-\beta$$

In R, we have the funtion `power.t.test`, which
can tell us the power

Für den T-Test können wir in R die folgende Funktion verwenden, die uns hier beispielsweise
die Frage beantwortet: "Wie viele Proben muss ich pro Gruppe (mindestens) nehmen,
um einen erwarteten Unterschied der Mittelwerte von 1 mit einer Standardabweichung von 1
in 80% der Fälle auch tatsächlich als solchen zu erkennen?"

```{r}
# show power.t.test

```

## Problems

### The Jelly Bean Problem (Multiple Testing)

```{r beans, fig.cap="(Quelle: Randall Munroe, https://xkcd.com/882/ )", include=FALSE, eval=TRUE}
knitr::include_graphics("img/significant.png")
```

Controlling the False Discovery Rate (FDA):

#### Bonferroni correction

```{r}
# show p.adjust
# with bonferroni
```

Jeder P-Value wird mutipliziert mit der Zahl der Tests

#### Benjamini-Hochberg procedure

- Sort all p-values in ascending order
- Choose a FDR $q$ and call the number of tests done $m$
- Find the largest p-value with:
  $p \leq iq/m$ with its index $i$.
- This is your new threshold for significance

```{r}
# show p.adjust for BH
```

### The Base Rate Fallacy

Example: mammogram

- Sensitivity = Power = true positive rate
- Specificity = true negative rate = $1-\alpha$

```{r}
# calculate 
total <- 1000
positives <- 10
negatives <- total - positives
sensitivity <- 0.9
specificity <- 1 - 0.08
true_positives  <- sensitivity * positives
false_positives <- (1 - specificity) * negatives
p_positive <- true_positives / (true_positives + false_positives)
```


```{r, echo = FALSE}
library(waffle)
theme_set(theme_void())

tibble(
  parts = c("positives", "negatives"),
  vals = c(positives, negatives)
) %>% 
  ggplot(aes(fill = parts, values = vals)) +
  geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("white", "red")) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "", override.aes = list(color = "black")))
```

```{r, echo = FALSE}
tibble(
  parts = c("detected positives",
            "not detected positives (false negatives)",
            "negatives"),
  vals = c(true_positives, positives - true_positives, negatives)
) %>% 
  ggplot(aes(fill = parts, values = vals)) +
  geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("darkred", "white", "red"))  +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "", override.aes = list(color = "black")))
```

```{r, echo = FALSE}
tibble(
  parts = c("detected positives",
            "not detected positives (false negatives)",
            "false positives",
            "true negatives"),
  vals = c(true_positives,
           positives - true_positives,
           round(false_positives),
           negatives)
) %>% 
  ggplot(aes(fill = parts, values = vals)) +
  geom_waffle(n_rows = 20) +
  coord_equal() +
  scale_fill_manual(values = c("darkred", "palevioletred1", "red", "white")) +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "", override.aes = list(color = "black")))
```

### Resources

- Statistics Done Wrong: https://www.statisticsdonewrong.com/
- Intuitive Biostatistics, also in the Uni-Bib: https://katalog.ub.uni-heidelberg.de/cgi-bin/titel.cgi?katkey=68260114&sess=050a1316767b181982c9bce94283e9ae&query=Intuitive%20Biostatistics
- https://www.graphpad.com/guides/prism/8/statistics/index.htm

## Exercises

### Show the following statements with simulations in R

#### Increasing the sample size n reduces the standard error of the mean with the square root of n.

- Draw 10 cells from the wt (or ko) vector
- calculate the mean
- write a function that does that
- run it 1000 times and save the result in a vector
- calculate the SD of the means
- now, draw 40 instead of 10 cells and repeat. How does the SD change?
- Write a function of n that does obove steps
- Feed the numbers from 1 to 100 to the function and
  plot the resulting SDs

#### A 95% confifence interval of a sample contains the true mean (of the population) with a probability of 95%

- draw 30 cells from the wt (or ko) vector
- Calculate the limits of the CI
- write a function that does that
- use `map` or `replicate` to get 1000 sets of limits
- Write a function that tests, if a set
  of limits contains the true mean
- Map this function over the list of limits
- How often to you optain `TRUE`?

### Show the following concepts with simulations in R

#### Sensitivity: With a true difference existing, how large is the probability to detect it (i.e. get a p-value <= 0.05) with a wilcoxon rank sum test?

- Draw a sample of 30 from the wt and the ko vector and test
  for a statistically significant difference
- Do this 1000 times. How many are statistically significant?
- What changes when you draw 10 instead of 30? Or 5?

#### Specificity: With **no** difference existing, what is the probabilty to detect one nonetheless?

- Imagine all cells are wt-cells
- Draw two samples of n = 30 from the wt-cells (i.e. the same population)
  and run a wilcoxon rank sum test
- Repeat 1000 times
- How are the p-values distributed?
- How often is the result statistically significant?

#### Resource for p-value Histograms

http://varianceexplained.org/statistics/interpreting-pvalue-histogram/


```{r}
knitr::opts_chunk$set(include = FALSE, eval=FALSE)
```

#### Solutions
  

```{r}
library(tidyverse)

read_csv("data/03_inclusion_bodies.csv") %>% attach()
```

```{r}
draw <- sample(wt, 10)
mean(draw)
```

```{r}
get_sample_mean <- function(x, n) {
  draw <- sample(x, n, replace = TRUE)
  mean(draw)
}
```

```{r}
get_sample_mean(wt, 10)
```

```{r}
N <- 1000
many_means <- map_dbl(1:N, ~ get_sample_mean(wt, 10) )
sd(many_means)
```

```{r}
many_means <- map_dbl(1:N, ~get_sample_mean(wt, 40))
sd(many_means)
```

```{r}
# explain default arguments and scoping
get_sd_of_many_means <- function(n, x, N = 1000) {
  many_means <- map_dbl(1:N, ~get_sample_mean(x, n))
  sd(many_means)
}
```

```{r}
n_max <- 100
sds_by_n <- map_dbl(1:n_max, get_sd_of_many_means, x = wt)
```

```{r}
plot(x = 1:n_max,
     y = sds_by_n, type = "p")

curve(sd(wt)/sqrt(x), add = TRUE, col = "red")
```

```{r}
# talk about difference between sample sd and population sd
```


#### Solutions

```{r}
test_samle_wilcox <- function(n) {
  draw_wt <- sample(wt, n, replace = TRUE)
  draw_ko <- sample(ko, n, replace = TRUE)
  wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value
}
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(30))
```

```{r}
hist(many_p_values)
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(10))
```

```{r}
hist(many_p_values)
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_samle_wilcox(4))
```

```{r}
hist(many_p_values)
```

```{r}
alpha = 0.05
mean(many_p_values <= alpha)
```

```{r}
# talk about power, effect size, difference between wilcox and t-test
```


#### Solutions

```{r}
test_same_wilcox <- function(n) {
  draw_wt <- sample(wt, n, replace = TRUE)
  draw_ko <- sample(wt, n, replace = TRUE)
  wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value
}
```

```{r}
many_p_values <- map_dbl(1:1000, ~test_same_wilcox(30))
```

```{r}
hist(many_p_values)
```

```{r}
alpha = 0.05
mean(many_p_values <= alpha)
```

```{r}
# this is what we would expect from p-values!
```



#### Solutions

```{r}
draw <- sample(wt, 30)
```

```{r}
test_results <- t.test(draw)
```

```{r}
test_results
```

```{r}
summary(test_results)
```

```{r}
str(test_results)
```

```{r}
test_results$conf.int
```

```{r}
get_sample_ci <- function(x, n) {
  draw <- sample(x, n, replace = TRUE)
  t.test(draw)$conf.int
}
```

```{r}
get_sample_ci(wt, 30)
```

```{r}
CIs <- map(1:1000, ~get_sample_ci(wt, 30))
```

```{r}
head(CIs)
```

```{r}
# explain list subsetting
CIs[1]
```

```{r}
CIs[[1]]
```

```{r}
CIs[[1]][1]
```


```{r}
test_ci <- function(limits, true_mean) {
  limits[1] < true_mean & limits[2] > true_mean
}
```


```{r}
# sidenote
# note difference between & and &&
c(TRUE, TRUE) &  c(FALSE, TRUE)
```

```{r}
c(TRUE, TRUE) && c(FALSE, TRUE)
```


```{r}
results <- map_lgl(CIs, test_ci, true_mean = mean(wt))
head(results)
```

```{r}
mean(results)
```

```{r}
# talk about organisation of code
# source("test.R")
```

#### Further notes

```{r}
# check out
?map
# for the subtle differences between:
map(1:100, ~ .x + 1)
# and
add_one <- function(x) x + 1
map(1:100, add_one)
# and
map_dbl(1:100, add_one)

# especially the meaning of ~ (speak: lambda)
map(1:5, ~ print("hi"))

# more examples
map(1:10, paste, "hi")
is_even <- function(x) x %% 2 == 0
map_lgl(1:10, is_even)

# Note: Often, you don't need a map function
# Because many functions in R are vectorised by default:
x <- 1:10
y <- 1:10
# thus, just write
x + y
# instead of
map2_dbl(x,y, `+`)
```


