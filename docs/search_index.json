[
["index.html", "Introduction to Data Science with R for Biochemists Prologue 0.1 Preparation for the Course 0.2 Structure of the Course 0.3 Content 0.4 Resources 0.5 Other Resources", " Introduction to Data Science with R for Biochemists Jannik Buhr 2020-11-22 Prologue 0.1 Preparation for the Course The course for master students will be held in English, as the concepts covered will directly transfer to the research you do and the papers you will most likely write with the working language in this domain being English. Before the course starts, please install both R and RStudio from the following links and bring your laptops to class: R: https://cran.r-project.org/ RStudio: https://www.rstudio.com/products/rstudio/download/#download Alternatively, if you can not install anything on your machine or would like to use the computers (Mac) provided in our classroom, sign up for RStudio Cloud. This way we are not dependent on the IT department for the correct installation of R, RStudio and additional packages. 0.2 Structure of the Course From January 10, 2020 to February 14 (6 fridays) Morning session from 10 ct to about 12:00. Afternoon session from 13:00 to about 15:00, but I will be there for further questions until max 17:00. 0.3 Content Einleitung Was ist dieses R? R und RStudio R als Taschenrechner .R-Dateien (Skripte) Variablen und arithmetische Operationen Wir machen es uns gemütlich in RStudio Einstellungen, Themes, etc. Project-based Workflow R Markdown Das Tidyverse (und andere Packages) Hilfe finden Die Community StackOverflow, GitHub, R4DS, Slack, Advanced R Arten von Daten Daten in der Wildnis Data in R Vector, matrix, array, list, data.frame (tibble) Data formats, Getting data into R Mein erster Plot Das letzte Kuchendiagramm Barplots Base R vs ggplot2 The grammar of graphics Scatterplots Tidy data Prinzip Daten importieren Data-Wrangling mit dplyr and tidyr Funktionale Programmierung (vs OOP) Funktionen schreiben FP vs. OOP Pure functions und Functional Programming Statistik Basics: sd, var, mean, median, correlation Histogramme, Verteilungen p-values t.test, Wilcoxon rank sum test, quisquared (ANOVA) Modelling and data fitting Lineare Regression Analyse modelr, broom \\(R^2\\), rmse, residuals, plots, non-linear regression Many models nested datframes, list colums map Funktionen This table of content is structured thematically, not chronologically. 0.4 Resources 0.4.1 Tidyverse R for Data Science (Wickham und Grolemund 2017) R4DS online Community RStudio Cheat Sheets! The Modern Dive (kim2019?) RStudio Education 0.4.2 R in general Advanced R (wickham2019?) Hands on Programming with R (grolemund2014?) R Packages (wickham2015?) Data Visualization: A Practical Introduction (healy2018?) Graph Cookbook (chang2013?) 0.4.3 Statistics Intuitive Biostatistics (motulsky2017?) Statistics Done Wrong (reinhart2015?) 0.4.4 Talks, Podcasts, Blogs David Robinson, YouTube 0.4.5 Misc Unglaublich niedliche Illustrationen (horst2019?) Happy Git with R 0.5 Other Resources Tidytuesday Tips for Working with Images in Rmarkdown Made with the help of these amazing packages (plus documentation): R Core Team (2019); Xie (2019a); Xie (2019b); Allaire u. a. (2019); Xie (2015) "],
["day1.html", "Day 1 And Go! 1.1 Basic Datastructures in R 1.2 The first Plots", " Day 1 And Go! 1.1 Basic Datastructures in R Because R is a programming language, we first look into some basic concepts and data structures before diving into the fun part. To test out everything we will learn today yourself, open RStudio, find the top left corner and click the little document with a plus icon. From the menu select “R Script”1. This chapter will jump into the basic concepts of the R programming language and is inspired by learnXinYminutes. It is completely normal that some of the concepts covered in this section will not appear obvious at first. Programming languages are not unlike human languages and the computer will not always understand what you want it to do unless you use exactly the right grammar and vocabulary. However, R will sometimes tell you, that it didn’t understand you through error messages and it is quite the norm to see them a lot. Even experienced programmers are very fond of this advice: Figure 1.1: Maybe the most important programming advice. 1.1.1 Executing Code in R The simplest use-case for any programming language is probably just to use it as a calculator (because that’s what computers are, big calculators). Type the following line in your R script and then hit Ctrl+Enter to send it to the R console. This is where your code gets executed. 1 + 1 ## [1] 2 You can also type directly in the console and then hit enter to execute, but in order to keep everything reproducible, we write everything in the R script file so that we can come back later and execute the same code to proof that it still works and that for example our data analysis for an experiment is valid. Your code can have comments to tell your future self why you wrote a piece of code the way you did. Any line starting with the number symbol # will be ignored by R. # This line will be ignored 43 - 1 # as will be the part after this #, but not before it ## [1] 42 1.1.2 Basic Datatypes Whole numbers (integer) 1L # denoted by L Numbers (numeric, double) 12 12.5 Complex numbers 1 + 3i # denoted by the small i for the imaginary part Text (character, string) &quot;Harry Potter&quot; Logical values (boolean, logical) TRUE FALSE Special types that mix with any other type: # NULL for no value NULL # NA for Not Assigned NA NA is contagious. Any computation involving NA will return NA (because R has no way of knowing the answer): NA + 1 ## [1] NA max(NA, 12, 1) ## [1] NA Factors (factor) for categorical data. # Factors can be ordered (like grade levels) or unordered (like gender) factor(c(&quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;d&quot;, &quot;m&quot;)) ## [1] m f f d m ## Levels: d f m 1.1.3 Variables Variables store data. You can think of them as a box with a name in which you can put stuff. Putting stuff in boxes works via “assignment” and the operator in R for that looks like this: &lt;-. You can either type it directly or use the handy RStudio shortcut Alt+Minus to insert one into your code. name &lt;- &quot;Kvothe&quot; x &lt;- 12 y &lt;- 14 Executing the above code will not give you any output, but when you use the name of the variable, you can see its content: x ## [1] 12 name ## [1] &quot;Kvothe&quot; And you can do operations with those variables: x + y ## [1] 26 NOTE Be careful about the order of execution! R enables you to work interactively and to execute the code you write in your script in any order with Ctrl+Enter, but when you execute (source) the whole script, it will be executed from top to bottom. Furthermore, code is not executed again automatically, if you change some dependency of the expression later on. So the second assignment to x doesn’t change y. x &lt;- 1 y &lt;- x + 1 x &lt;- 1000 y ## [1] 2 Variable names can contain letters (capitalization matters), numbers (but not as the first character) and underscores _.2 # snake_case main_character_name &lt;- &quot;Kvothe&quot; # or camelCase bookTitle &lt;- &quot;The Name of the Wind&quot; # you can have numbers in the name x1 &lt;- 12 x2 &lt;- 13 # There are other ways to assign variables x3 = 12 # this can be confused with setting parameters in function calls 13 -&gt; x4 Typing and executing the name of a variable will implicitly call the print function on it, which displays its content in the console. bookTitle # == print(mainCharacterName) 1.1.4 Math Operators An operator is a function that is called inline, so it stands between its arguments, like +. R can do standard math and much more. x1 + x2 12 - 13 13 * 12 10 / 5 12 %/% 5 # integer division 12 %% 5 # modulo (the rest of integer division) # NaN, Not a Number 0 / 0 10 / 0 Inf -Inf 1.1.5 Functions and Vectors Functions are the main workhorses of our data analysis. There are mathematical functions, like sin, cos etc. sin(x = 0) ## [1] 0 The sin function takes just one argument x and returns its sine. Note, that the = inside the function parenthesis gives x = 0 to the function and has nothing to do with any x defined outside of the function. The parameter x used in the function is separate from any x you might have defined outside of the function. E.g. x &lt;- 10 # The sin function uses an x = 0 sin(x = 0) ## [1] 0 # But the other x is still 10! x ## [1] 10 If you want to know more about a function in R, execute ? with the function name or press F1 with your mouse over the function. ?sin This is important {.note} We can pass arguments by name, or by order of appearance. This allows us to simplify the previous expression. sin(x = 12) ## [1] -0.5365729 sin(12) ## [1] -0.5365729 The basic datatypes in R are all vectors, which means they can contain more than one entry. You can create a vector by combining things of the same data type with the function c for combine. x &lt;- c(1,2,3,4,5,6) x ## [1] 1 2 3 4 5 6 The basic mathematical operations in R are vectorized by default i.e. they are performed on every element of the vector. Here, every element is multiplied by 2 and the result printed to the console. x * 2 ## [1] 2 4 6 8 10 12 The original vector x was not changed in doing so. x ## [1] 1 2 3 4 5 6 But we could have by assigning the result back to x, thus overwriting its previous content. The right hand side (RHS) is executed first: x &lt;- x * 2 Now x changed: x ## [1] 2 4 6 8 10 12 This line performs vector addition: c(1,2,3) + c(4,5,6) ## [1] 5 7 9 The : operator and seq allow us to create vectors of numbers quite easily: x &lt;- 1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 evenNumbers &lt;- seq(from = 0, to = 10, by = 2) evenNumbers ## [1] 0 2 4 6 8 10 logical comparisons are vectorized as well return logical (boolean) vectors 12 &lt; 13 ## [1] TRUE answer &lt;- 12 &gt; 13 answer ## [1] FALSE Most things can be compared main_character_name == &quot;Kvothe&quot; # TRUE ## [1] TRUE main_character_name == bookTitle # FALSE ## [1] FALSE 1 == 1 # TRUE ## [1] TRUE ! means not. 1 != 2 ## [1] TRUE There is logical and and or: # &quot;and&quot; =&gt; both need to be TRUE to get TRUE TRUE &amp; FALSE ## [1] FALSE # &quot;or&quot; =&gt; only one needs to be TRUE TRUE | FALSE | FALSE ## [1] TRUE TRUE | TRUE ## [1] TRUE They also work element-wise. a &lt;- c(TRUE, FALSE, TRUE) b &lt;- c(FALSE, FALSE, TRUE) a | b ## [1] TRUE FALSE TRUE Periodic reminder: error messages are your friends and completely normal. This error for example tells you that R expected another argument to the c function because you added another comma: c(1,2,) ## Error in c(1, 2, ): argument 3 is empty There are errors, messages and warnings. Your code will not run to the end if R finds errors, but warnings and messages are Rs way of telling you, that your code ran but you might want to double-check if it is doing what you expected it to be doing. warning(&quot;Warnings are often fine!&quot;) ## Warning: Warnings are often fine! message(&quot;I am a message&quot;) ## I am a message Combining different datatypes into a vector with the c function will coerce them into the same type, choosing the one that allows the most operations. It can also be done explicitly. # implicit paste(&quot;hello&quot;, &quot;world&quot;, 13) ## [1] &quot;hello world 13&quot; # explicit as.character(13) ## [1] &quot;13&quot; 1.1.5.1 Writing Functions Everything that does something is a function, everything that exists is an object. We can write our own functions (like sin) greet &lt;- function(name) { # The variable &quot;text&quot; only exists # in the context of this function. # It is not visible to the outside world. text &lt;- paste(&quot;Hello &quot;, name, &quot;!&quot;, sep = &quot;&quot;) return(text) } Here, I am assigning return value of the function to the variable result and then look at the content of result result &lt;- greet(&quot;Jannik&quot;) result ## [1] &quot;Hello Jannik!&quot; addOne &lt;- function(x) { return(x + 1) } If your function fits on one line, you can leave out the curly braces {}. addOne &lt;- function(x) x + 1 If you don’t explicitly use the return() statement inside of the function definition, the function will return the last evaluated line, which is why both versions of addOne above are valid R code and do the same thing. We use a function by calling it with the required arguments / parameters: addOne(x = 1) ## [1] 2 Instead of explicitly stating the parameters with their names when calling the function, we can also rely on the order in which a function is expecting the arguments: addOne(1) # the same as addOne(x = 1) ## [1] 2 1.1.6 Datastructures 1.1.6.1 1D, same datatype: vector x &lt;- c(&quot;harry&quot;, &quot;ron&quot;, &quot;hermione&quot;) x ## [1] &quot;harry&quot; &quot;ron&quot; &quot;hermione&quot; Getting specific elements of a vector (or other data structure) is called subsetting: # We ask for specific elements by subsetting with square brackets [] # (Note that R starts counting from 1) x[3] ## [1] &quot;hermione&quot; x[1:2] ## [1] &quot;harry&quot; &quot;ron&quot; # subsetting with logical vector logicalVector &lt;- c(TRUE, FALSE, TRUE) ## the next 3 statements do the same thing x[logicalVector] ## [1] &quot;harry&quot; &quot;hermione&quot; # or with indices x[c(1,3)] ## [1] &quot;harry&quot; &quot;hermione&quot; # which x[which(logicalVector)] ## [1] &quot;harry&quot; &quot;hermione&quot; There is a number of useful functions for vectors like: x &lt;- 1:100 # grab just the first or last few entries in the vector, head(x) ## [1] 1 2 3 4 5 6 tail(x) ## [1] 95 96 97 98 99 100 # or figure out if a certain value is in the vector any(x == 2) ## [1] TRUE which(x == 2) ## [1] 2 x[which(x == 2)] ## [1] 2 # If an index &quot;goes over&quot; you&#39;ll get NA: x[101] ## [1] NA # You can find the length of your vector with length() length(x) ## [1] 100 # You can perform operations on entire vectors or subsets of vectors # [] for subsetting, get a piece of a vector (or list, or anything else) # () for order of application and more importantly # for calling functions result &lt;- greet(&quot;jannik&quot;) x[x %% 2 == 0] ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 ## [20] 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 ## [39] 78 80 82 84 86 88 90 92 94 96 98 100 # and R has many built-in functions to summarize vectors # mean, min, max, sum mean(x) ## [1] 50.5 min(x) ## [1] 1 max(x) ## [1] 100 sum(x) ## [1] 5050 c(1,2,3, &quot;harry&quot;, TRUE) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;harry&quot; &quot;TRUE&quot; c(1, 2, 3, TRUE) ## [1] 1 2 3 1 mean(x %% 2 == 0) ## [1] 0.5 1.1.6.2 1D, different datatypes: list Lists # create list l1 &lt;- list(1, &quot;harry&quot;, TRUE, c(1,2,3) ) # single [] subsetting l1[1] ## [[1]] ## [1] 1 l1[1:2] ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;harry&quot; # double [[]] subsetting l1[[1]] ## [1] 1 # lists and vectors can have names names(l1) &lt;- c(&quot;number&quot;, &quot;potter&quot;, &quot;bool&quot;, &quot;vec&quot;) l1 ## $number ## [1] 1 ## ## $potter ## [1] &quot;harry&quot; ## ## $bool ## [1] TRUE ## ## $vec ## [1] 1 2 3 # using the names for subsetting l1$number # same as l1[[1]] ## [1] 1 l1[[&quot;number&quot;]] ## [1] 1 l1[&quot;number&quot;] ## $number ## [1] 1 1.1.6.3 2D, same Datentyp: matrix # matrix # matrix(1:9) # matrix(1:9, nrow = 3, byrow = TRUE) m &lt;- matrix(1:9, nrow = 3) # Ask for the first row m[1, ] ## [1] 1 4 7 # the first column m[, 1] ## [1] 1 2 3 # the element in the top right corner m[1,3] ## [1] 7 # Perform operation on the first column m[,1] * 2 ## [1] 2 4 6 m[,1] &lt;- m[,1] * 2 m ## [,1] [,2] [,3] ## [1,] 2 4 7 ## [2,] 4 5 8 ## [3,] 6 6 9 # Transpose the whole matrix t(m) ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 4 5 6 ## [3,] 7 8 9 # Matrix multiplication m %*% m ## [,1] [,2] [,3] ## [1,] 62 70 109 ## [2,] 76 89 140 ## [3,] 90 108 171 m &lt;- m * 2 # cbind() sticks vectors together column-wise to make a matrix cbind( 1:3, c(1,2,42) ) ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 42 m ## [,1] [,2] [,3] ## [1,] 4 8 14 ## [2,] 8 10 16 ## [3,] 12 12 18 # rbind() sticks vectors together row-wise to make a matrix rbind( 1:3, c(1,2,42) ) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 42 1.1.6.4 2D, different datatypes: data.frame # data.frame students &lt;- data.frame( name = c(&quot;harry&quot;, &quot;ron&quot;, &quot;cedric&quot;), year = c(2, 2, 3), alive = c(TRUE, TRUE, FALSE), stringsAsFactors = FALSE ) students ## # A tibble: 3 x 3 ## name year alive ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 harry 2 TRUE ## 2 ron 2 TRUE ## 3 cedric 3 FALSE Useful functions for dataframes # nrow, ncol, dim nrow(students) ## [1] 3 ncol(students) ## [1] 3 dim(students) ## [1] 3 3 # parameter stringsAsFactors # There are many ways to subset data frames # See if you can find out what each of these lines does # dollar students$name ## [1] &quot;harry&quot; &quot;ron&quot; &quot;cedric&quot; # vector students[1:2] ## # A tibble: 3 x 2 ## name year ## &lt;chr&gt; &lt;dbl&gt; ## 1 harry 2 ## 2 ron 2 ## 3 cedric 3 students[1, 1] ## [1] &quot;harry&quot; # character students[&quot;name&quot;] ## # A tibble: 3 x 1 ## name ## &lt;chr&gt; ## 1 harry ## 2 ron ## 3 cedric students$year &lt;- students$year + 3 students ## # A tibble: 3 x 3 ## name year alive ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 harry 5 TRUE ## 2 ron 5 TRUE ## 3 cedric 6 FALSE students &lt;- cbind(students, stuff = c(1,2,3)) students[students$name == &quot;cedric&quot;, &quot;year&quot;] &lt;- 8 students[students$alive, ] ## # A tibble: 2 x 4 ## name year alive stuff ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 harry 5 TRUE 1 ## 2 ron 5 TRUE 2 Almost as useful as ?: the function str for structure. str(students) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ name : chr &quot;harry&quot; &quot;ron&quot; &quot;cedric&quot; ## $ year : num 5 5 8 ## $ alive: logi TRUE TRUE FALSE ## $ stuff: num 1 2 3 1.2 The first Plots 1.2.1 Scatterplot x &lt;- 1:20 y &lt;- x^2 plot(x, y) x &lt;- 1:20 y &lt;- x + rnorm(length(x)) plot(x,y) With a linear regression # model lm model &lt;- lm(y ~ x) # plot plot(x,y, xlab = &quot;This is x&quot;, main =&quot;Hello&quot;, col = &quot;darkgreen&quot;) # abline abline(model, col = &quot;red&quot;) 1.2.2 Curve curve(sin, -5, 5) 1.2.3 Histogram y &lt;- rnorm(1000) hist(y) 1.2.4 The last piechart you will ever need. cols &lt;- c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;) pie( c(280, 60, 20), init.angle = -50, col = cols, border = NA, labels = NA ) legend(1, 1, xjust = 0.5, yjust = 1, fill = cols, border = NA, legend = c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;) ) 1.2.5 Saving Plots By executing a piece of code that contains a plot, the plot is send to a so called graphics device. Think of it as a printer for your plots. By default this is the plot pane of RStudio (lower right). If you want to save your base-R plot to a file, you first open a different graphics device that redirects your plot for example to a png-file, then run the plot code and then close the device to finalize the printing step. png(filename = &quot;myFirstPlot.png&quot;) curve(sin, -5, 5, main = &quot;So much sin!&quot;) dev.off() For power users: the shortcut is Ctrl+Shift+N↩︎ They can also contain dots (.), but it is considered bad practice because it can lead to some confusing edge cases.↩︎ "],
["day2.html", "Day 2 Bring on the Data! 2.1 Project-based Workflow 2.2 Packages: The Tidyverse 2.3 Exercise from day 1 2.4 Reading data 2.5 The Pipe and dplyr Verbs 2.6 But what is Probability? 2.7 Exercise: Transfer to new Data", " Day 2 Bring on the Data! 2.1 Project-based Workflow Whenever you work on a new project (like a lab analysis), create a new RStudio Project. You can find the quick menu for this in the top right corner of RStudio. Projects automatically set your so-called working directory to this project folder. This is where R will search for files and save files to. Every file path should be relative to this working directory. This also enables the RStudio autocompletion to help you find your files. Inserting a pair of quotations marks (\") into your code and pressing Tab or Ctrl+Space shows you the files and folders in your working directory. 2.1.1 Workflow of a Data Analysis A typical workflow according to Hadley Wickam, Chief Scientist at RStudio: Figure 2.1: Source: Wickham und Grolemund (2017) 2.1.2 Communication! Your thoughts during data analysis matter (To future-you and others)! This is one of the main reasons I use Rmarkdown instead of R scripts for teaching, as well as regular data analysis. This .Rmd format mixes text with code chunks. You can create your own in the top left corner of RStudio. Choose “R markdown” and press OK. This will present you with an example document showcasing some of the possibilities. A plethora of information can be found in the official Rmarkdown guide (Allaire u. a. 2019). Text is interpreted as markdown, with special characters to mark text as bold, italics, a heading etc. and text in so called code chunks is interpreted as (R) code. You can insert R code chunks with the shortcut Ctrl+Alt+I or the button in the upper right corner of the code editor window. To execute (= send to the console) a piece of code in a chunk hit the familiar Ctrl+Enter. To Execute Hitting the knit button will run the whole document and produce a report in the specified format (html, docx, pdf and many others). 2.1.3 Important RStudio-Settings For the sake of reproducibility, please set these settings (Tools -&gt; Global Options) in your RStudio. Figure 2.2: RStudio Settings 2.2 Packages: The Tidyverse This is where a lot of the power of R comes from. Packages are… Installing packages # install tidyverse install.packages(&quot;tidyverse&quot;) Load packages: # load tidyverse library(tidyverse) ## ── Attaching packages ──────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() The tidyverse is 2.3 Exercise from day 1 2.3.1 Questions What do we get when we execute c(\"12\", 13, 14)[2] + 1? Try do predict what will happen before executing the code. Why does it happen that way? Make a vector x with the numbers from 1 to 10 Make a vector y with the same content as x Plot both vectors as points (y ~ x) hint: use the plot function Do a linear regression and add it as a line to the plot How can we find out more about the linear regression object? 2.3.2 Answers c(&quot;12&quot;, 13, 14)[2] + 1 vec &lt;- c(&quot;12&quot;, 13, 14) vec[2] vec[2] + 1 Ctrl+Shift+Enter to run the whole chunk x &lt;- 1:10 y &lt;- x # plot(x, y) plot(y ~ x) linearModel &lt;- lm(y ~ x) abline(linearModel) 2.4 Reading data The readr-package is part of the tidyverse, which we already loaded above with library(tidyverse). Take your favourite table calculation program (Excel, Libre Office). We create a list of course participants and at the same time note the row in which they are sitting and their hair length. We can save the file as .csv (Comma Separated Values), which is a plain text file and can be opened with any text editor. As a destination we create a folder in our project called data, to keep everything organized. read_csv(&quot;data/students.csv&quot;) You might have a German Excel version… Because the German language uses the comma as a decimal separator, it can’t use the comma to separate entries in a table as well. This is why the German version of a “csv” produced by Excel is actually semicolon (;) separated. But there is an easy fix for that: read_csv2(&quot;data/students.csv&quot;) Both read_csv... functions are just special cased of read_delim, where any character can be specified to separate entries, not just commata or semicolons: # read_csv equals read_delim(&quot;data/students.csv&quot;, delim = &quot;,&quot;) # read_csv2 equals read_delim(&quot;data/students.csv&quot;, delim = &quot;;&quot;) If you are not sure what function to use, open the file in a regular text editor like notepad and have a look. You can also read straight from excel files. readxl::read_excel(&quot;data/students.xlsx&quot;) Up until now we only looked at the results of calling the read-functions but we didn’t assign the data to a variable so that we can use it. Let us do so now: students &lt;- read_csv(&quot;data/students.csv&quot;) ## Parsed with column specification: ## cols( ## name = col_character(), ## row = col_double(), ## hairlength = col_character() ## ) You can view your data in a new tab in RStudio. View(students) Or simply look at the content of our variable students: students ## # A tibble: 10 x 3 ## name row hairlength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 jannik 0 s ## 2 ulrich 1 s ## 3 lea 1 l ## 4 melanie 1 l ## 5 judith 1 l ## 6 maria 1 l ## 7 alex 2 s ## 8 christina 2 l ## 9 jan 2 s ## 10 pamina 2 l Writing data works analogous to that. write_csv(students, path = &quot;data/students2.csv&quot;) Now what do we do with the data? 2.5 The Pipe and dplyr Verbs The dplyr package and the pipe (%&gt;%) 2.5.1 The Pipe Data needs to be processed. Functions that process data are used sequentially. The common way (in other languages) to write this looks about like this: First we define some toy functions. addOne &lt;- function(x) x + 1 standardize &lt;- function(x) x / max(x) callOutResult &lt;- function(x) paste(&quot;Your result is&quot;, x) Then we use them with our example data, which is just the numbers from 1 to 5. data &lt;- 1:5 newData &lt;- addOne(data) evenNewerData &lt;- standardize(newData) callOutResult(evenNewerData) ## [1] &quot;Your result is 0.333333333333333&quot; &quot;Your result is 0.5&quot; ## [3] &quot;Your result is 0.666666666666667&quot; &quot;Your result is 0.833333333333333&quot; ## [5] &quot;Your result is 1&quot; This is not fun to write! We might want to overwrite our original data instead of creating a new variable for every step. data &lt;- 1:5 data &lt;- addOne(data) data &lt;- standardize(data) callOutResult(data) ## [1] &quot;Your result is 0.333333333333333&quot; &quot;Your result is 0.5&quot; ## [3] &quot;Your result is 0.666666666666667&quot; &quot;Your result is 0.833333333333333&quot; ## [5] &quot;Your result is 1&quot; This is a lot of repetition! Who wants to type data that often? Maybe we turn to math notation with brackets? data &lt;- 1:5 callOutResult(standardize(addOne(data))) ## [1] &quot;Your result is 0.333333333333333&quot; &quot;Your result is 0.5&quot; ## [3] &quot;Your result is 0.666666666666667&quot; &quot;Your result is 0.833333333333333&quot; ## [5] &quot;Your result is 1&quot; This is certainly not pretty. Enter: The pipe %&gt;%. Insert a pipe with Ctrl+Shift+M (Or manually type it out) The pipe enables us to pass the result of one function on to the next function. With pure functions g and f, the following semi-mathematical equation holds true: f(g(x)) = x %&gt;% g() %&gt;% f() data &lt;- 1:5 data %&gt;% addOne() %&gt;% standardize() %&gt;% callOutResult() ## [1] &quot;Your result is 0.333333333333333&quot; &quot;Your result is 0.5&quot; ## [3] &quot;Your result is 0.666666666666667&quot; &quot;Your result is 0.833333333333333&quot; ## [5] &quot;Your result is 1&quot; We can also write each step on a new line (because R doesn’t care about new lines). data %&gt;% addOne() %&gt;% standardize() %&gt;% callOutResult() ## [1] &quot;Your result is 0.333333333333333&quot; &quot;Your result is 0.5&quot; ## [3] &quot;Your result is 0.666666666666667&quot; &quot;Your result is 0.833333333333333&quot; ## [5] &quot;Your result is 1&quot; For functions with more than one argument, the pipe inserts the stuff from the left hand side (LHS) as the first argument to the function! addNumber &lt;- function(x, number) x + number data %&gt;% addNumber(3) ## [1] 4 5 6 7 8 2.5.2 Overview of the dplyr Verbs These are the main functions we use to transform and process our data after we brought it into R with the readr package. These functions work on data.frames, or the slightly more modern version of the data.frame that the tidyverse calls a tibble (like table). These functions are: select filter arrange mutate summarise count And additionally the adverb: group_by 2.5.3 select Select columns of a data.frame (or tibble). Remember the “base-R” way to select columns? students[, c(&quot;name&quot;, &quot;row&quot;)] The select function replaces this: select(students, name, row) # or in a pipe students %&gt;% select(name, row) 2.5.4 filter Filter a tibble based on conditions applied to columns. Remember the old way to subset? students[students$row == 1, ] The new syntax is more straightforward: students %&gt;% filter(row == 1) And can be combined in pipes: students %&gt;% filter(row == 1) %&gt;% select(name) This new expression gives the same result as the base-R way of writing students[students$row == 1, 1] but is easier to read. The conditions in filter stack. students %&gt;% filter(row == 2, hairlength == &quot;l&quot;) ## # A tibble: 2 x 3 ## name row hairlength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 christina 2 l ## 2 pamina 2 l We can also combine conditions with or ( | ) students %&gt;% filter(row == 2 | hairlength == &quot;l&quot;) ## # A tibble: 8 x 3 ## name row hairlength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 lea 1 l ## 2 melanie 1 l ## 3 judith 1 l ## 4 maria 1 l ## 5 alex 2 s ## 6 christina 2 l ## 7 jan 2 s ## 8 pamina 2 l One thing to note: As of now, we have not changed the students data! because we only looked at the output of the functions in the console and did not assign anything (with &lt;- ). 2.5.5 mutate To Change a column or add a new one e.g. the length of your names. Last week in base R, adding a column worked like that: students$nameLength &lt;- str_length(students$name) But with the mutate function, it gets more readable. Note, that we do not refer to students$name explicitly inside of mutate. mutate knows that name is referring to a column of the tibble passed into the function from the left. Furthermore, there is an equal sign (=) in there, because everything inside of mutate is a parameter to the function, it is not an assignment (&lt;-). students %&gt;% mutate(nameLength = str_length(name)) We might need this modified/enhanced data later, so let’s overwrite the variable students: students &lt;- students %&gt;% mutate(nameLength = str_length(name)) head(students) ## # A tibble: 6 x 4 ## name row hairlength nameLength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 jannik 0 s 6 ## 2 ulrich 1 s 6 ## 3 lea 1 l 3 ## 4 melanie 1 l 7 ## 5 judith 1 l 6 ## 6 maria 1 l 5 2.5.6 arrange Who has the longest name? students %&gt;% arrange(desc(nameLength)) %&gt;% head() ## # A tibble: 6 x 4 ## name row hairlength nameLength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 christina 2 l 9 ## 2 melanie 1 l 7 ## 3 jannik 0 s 6 ## 4 ulrich 1 s 6 ## 5 judith 1 l 6 ## 6 pamina 2 l 6 This could have been achieved with filter as well: students %&gt;% filter(nameLength == max(nameLength)) ## # A tibble: 1 x 4 ## name row hairlength nameLength ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 christina 2 l 9 Note that the dplyr functions always return tibbles, not the raw vectors. But we can pull out a tibble column as a vector: students %&gt;% filter(nameLength == max(nameLength)) %&gt;% pull(name) ## [1] &quot;christina&quot; 2.5.7 count How many are in the first row? students %&gt;% count(row) ## # A tibble: 3 x 2 ## row n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 1 ## 2 1 5 ## 3 2 4 How many based on hairlength? students %&gt;% count(row, hairlength) ## # A tibble: 5 x 3 ## row hairlength n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 0 s 1 ## 2 1 l 4 ## 3 1 s 1 ## 4 2 l 2 ## 5 2 s 2 2.5.8 summarise What is the mean name-length? In base-R, we would have operated on the column of the tibble extracted as a vector with he $-operator. mean(students$nameLength) ## [1] 5.5 But in the tidyverse, we can work directly with the tibble. students %&gt;% summarise(nameLength = mean(nameLength)) ## # A tibble: 1 x 1 ## nameLength ## &lt;dbl&gt; ## 1 5.5 2.5.9 group_by und summarise Do participants in the first row have longer names? group_by is an adverb because by itself, it doesn’t do anything to the data, but it changes the way the other dplyr verbs operate. Any function that summarizes a vector, like mean, sum, max, now operates on the groups. students %&gt;% group_by(row) %&gt;% summarise(nameLengh = mean(nameLength)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## row nameLengh ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6 ## 2 1 5.4 ## 3 2 5.5 students %&gt;% group_by(row, hairlength) %&gt;% summarise(nameLength = mean(nameLength)) %&gt;% arrange(desc(nameLength)) ## `summarise()` regrouping output by &#39;row&#39; (override with `.groups` argument) ## # A tibble: 5 x 3 ## row hairlength nameLength ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 l 7.5 ## 2 0 s 6 ## 3 1 s 6 ## 4 1 l 5.25 ## 5 2 s 3.5 Do people with long hair tend to sit in the first row? students %&gt;% filter(row != 0) %&gt;% count(row, hairlength) ## # A tibble: 4 x 3 ## row hairlength n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 l 4 ## 2 1 s 1 ## 3 2 l 2 ## 4 2 s 2 2.5.9.1 Sidenotes pull pulls out a column of a tibble as a vector. students %&gt;% pull(name) ## [1] &quot;jannik&quot; &quot;ulrich&quot; &quot;lea&quot; &quot;melanie&quot; &quot;judith&quot; &quot;maria&quot; ## [7] &quot;alex&quot; &quot;christina&quot; &quot;jan&quot; &quot;pamina&quot; 2.5.10 Moving on to Probability The question we might ask now is: If every person were randomly seated, What would be the probability to find that many people (or more) with long hair in the first row? 2.6 But what is Probability? There are two concepts of probability (\\(P\\)): Probability inside your head: strength of belief; may vary among people Probability „out there“: long-term frequency of an event; can be empirically measured or predicted from a model (motulsky2017?). We will be concerned with the latter. 2.6.1 Example: Categorical / discrete data: Drawing (blindly) from a hat. students %&gt;% count(row, hairlength) ## # A tibble: 5 x 3 ## row hairlength n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 0 s 1 ## 2 1 l 4 ## 3 1 s 1 ## 4 2 l 2 ## 5 2 s 2 students %&gt;% filter(row != 0) %&gt;% count(hairlength) ## # A tibble: 2 x 2 ## hairlength n ## &lt;chr&gt; &lt;int&gt; ## 1 l 6 ## 2 s 3 Note This example uses a lot of base-R functions and less tidyverse style. Try to dicern between the use of normal vectors like in the following code and tibbles as observed in the tidyverse. We create a “hat” of hairlengths with the numbers observed in our course. hat &lt;- c( rep(&quot;l&quot;, 6), rep(&quot;s&quot;, 3) ) # rep stands for repeat hat ## [1] &quot;l&quot; &quot;l&quot; &quot;l&quot; &quot;l&quot; &quot;l&quot; &quot;l&quot; &quot;s&quot; &quot;s&quot; &quot;s&quot; And sample / draw blindly from said hat sample(hat, 1) ## [1] &quot;s&quot; Now we sample the same number of hairlenghts from the hat as the number of people sitting in the first row: firstRow &lt;- sample(hat, 5) This is representative of people coming into the room and getting seated randomly. Note that the default for sample is replace = FALSE (see ?sample), so the same person can only be seated once. It does not return to the hat to be drawn again. How many in this sample have long hair? sum(firstRow == &quot;l&quot;) ## [1] 4 What fraction? # this works because `mean` converts TRUE to 1 and FALSE to 0 mean(firstRow == &quot;l&quot;) ## [1] 0.8 We can simulate this draw a bunch of times, as if people were coming into the room over and over again, each time getting seated randomly. For this, we use a for loop. This loop executes the code between the curly braces once for every element of the vector 1:5. The current element of every iteration is called i in this case. for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 We use this concept to simulate drawing 1000 times, each time calculating the number of people with long hair in the first row. N &lt;- 1000 # create vector for the sum from each draw results &lt;- 1:N # assign the results in a loop for (i in 1:N) { draw &lt;- sample(hat, 5) results[i] &lt;- sum(draw == &quot;l&quot;) } The histogram shows us, how often each number appears in our results. hist(results, breaks = 0:5) How surprised should we be, that in our real world data, we found this number of long haired people in the first row? Well, let us check how often we got that many or more in our simulated data. # -&gt; Calculate probability for random event # sum greater than or equal to observed frequency / length mean(results &gt;= 4) ## [1] 0.445 So in 44.5 % of cases in our simulation, we got 4 or more people with long hair in the first row. This means, that the observed result is quite likely to occur just by chance. We can not prove that it did indeed resulted just from chance alone, but we can show that if it is just random chance, we are not very surprised to see a result like this in our data. 2.6.2 P-Values P-Values were introduced in the 1920s by Ronald Fisher: “The P value is defined as the probability, under the assumption of no effect or no difference (the null hypothesis), of obtaining a result equal to or more extreme than what was actually observed.” \\(-\\) (Original: Statistical Methods for Research Workers) (fisher1990?) By convention: p ≤ 0.05 is called “significant”. In other words, a p-Value is… “… a measure of how surprised you should be if there is no actual difference […], but you got data suggesting there is” \\(-\\) Alex Reinhart (reinhart2015?) We can calculate the exact p-value of our example with the hypergeometric distribution. # note that it calculates cumulative probabilities! # default: P(X &lt;= x) 1 - phyper(q = 3, m = 6, n = 3, k = 5) ## [1] 0.4047619 It is quite close to our simulation! And had we used a higher N, like 1’000’000, it would of course be even closer. The hypergeometric distribution for for our example looks as follows barplot(dhyper(x = 0:5, m = 6, n = 3, k = 5), names.arg = 0:5) Figure 2.3: Hypergeometric distribution for people with long hair in the first row with random seating. 2.7 Exercise: Transfer to new Data Starwars, a dataset that comes with the tidyverse. ?starwars In a new Rmarkdown document: Preview the dataset Select the columns name, height, mass, gender Who is the heaviest? Convert height from cm to m Which gender is taller on average in starwars? Hint: use group_by and summarise You will need the argument na.rm = TRUE in mean() Simulate drawing 81 characters (or rather genders) from a hat. Repeat this 1000 times (with a for-loop). How often do you obtain 62 or more male characters? How surprised should we be about the data? Calculate an exact p-value for the observed frequency note: use pbinom instead of phyper to sample WITH replacement Knit the document into a report 2.7.1 Solutions Live on Friday 2.7.1.1 First, we load packages and Data library(tidyverse) starwars %&gt;% select(name, mass, height, gender) %&gt;% head() ## # A tibble: 6 x 4 ## name mass height gender ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 Luke Skywalker 77 172 masculine ## 2 C-3PO 75 167 masculine ## 3 R2-D2 32 96 masculine ## 4 Darth Vader 136 202 masculine ## 5 Leia Organa 49 150 feminine ## 6 Owen Lars 120 178 masculine Sidenote: to put a column in the front: starwars %&gt;% select(gender, everything() ) %&gt;% head() ## # A tibble: 6 x 14 ## gender name height mass hair_color skin_color eye_color birth_year sex ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 mascu… Luke… 172 77 blond fair blue 19 male ## 2 mascu… C-3PO 167 75 &lt;NA&gt; gold yellow 112 none ## 3 mascu… R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none ## 4 mascu… Dart… 202 136 none white yellow 41.9 male ## 5 femin… Leia… 150 49 brown light brown 19 fema… ## 6 mascu… Owen… 178 120 brown, gr… light blue 52 male ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; Height is not in SI units! But we love SI units! So we convert height from cm to meter. starwars &lt;- starwars %&gt;% mutate(height = height / 100) View(starwars) starwars %&gt;% arrange(desc(mass)) %&gt;% head() ## # A tibble: 6 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jabb… 1.75 1358 &lt;NA&gt; green-tan… orange 600 herm… mascu… ## 2 Grie… 2.16 159 none brown, wh… green, y… NA male mascu… ## 3 IG-88 2 140 none metal red 15 none mascu… ## 4 Dart… 2.02 136 none white yellow 41.9 male mascu… ## 5 Tarf… 2.34 136 brown brown blue NA male mascu… ## 6 Owen… 1.78 120 brown, gr… light blue 52 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% filter(mass == max(mass, na.rm = TRUE)) %&gt;% head() ## # A tibble: 1 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Jabb… 1.75 1358 &lt;NA&gt; green-tan… orange 600 herm… mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% filter(mass == max(mass, na.rm = TRUE)) %&gt;% pull(name) ## [1] &quot;Jabba Desilijic Tiure&quot; Who is the heaviest? Convert height from cm to m Which gender is taller on average in starwars? Hint: use group_by and summarise You will need the argument na.rm = TRUE in mean() starwars %&gt;% group_by(gender) %&gt;% summarise(height = mean(height, na.rm = TRUE), N = n()) %&gt;% head() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 3 ## gender height N ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 feminine 1.65 17 ## 2 masculine 1.77 66 ## 3 &lt;NA&gt; 1.81 4 Simulate drawing 81 characters (or rather genders) from a hat. Repeat this 1000 times (with a for-loop). How often do you obtain 62 or more male characters? How surprised should we be about the data? Calculate an exact p-value for the observed frequency note: use pbinom instead of phyper to sample WITH replacement Knit the document into a report hat &lt;- c(&quot;male&quot;, &quot;others&quot;) sample(hat, 84, replace = TRUE, prob = c(0.49, 0.51)) ## [1] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;male&quot; ## [9] &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;others&quot; &quot;male&quot; ## [17] &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;others&quot; &quot;others&quot; ## [25] &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; ## [33] &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; ## [41] &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; ## [49] &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; ## [57] &quot;others&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;others&quot; &quot;others&quot; ## [65] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; ## [73] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;others&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; ## [81] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;male&quot; hat &lt;- c(&quot;male&quot;, &quot;others&quot;) results &lt;- 1:1000 for (i in 1:1000) { draw &lt;- sample(hat, 84, replace = TRUE, prob = c(0.49, 0.51)) results[i] &lt;- sum(draw == &quot;male&quot;) } hist(results, breaks = 40) sum(results &gt;= 62) ## [1] 0 mean(results &gt;= 62) ## [1] 0 pbinom(q = 62 - 1, size = 84, prob = 0.49, lower.tail = FALSE) ## [1] 3.176077e-06 Seeing Theory "],
["day3.html", "Day 3 Tidy Data, Visualization and Distributions 3.1 Recap of day 2 3.2 Visualization with ggplot2 3.3 Visualization is Key 3.4 Distributions 3.5 The Datasaurus Dozen 3.6 Exercises", " Day 3 Tidy Data, Visualization and Distributions We will be using functions and data from the tidyverse today, so lets load it. It is good practice to have all library loading and data import steps at the beginning of your document. library(tidyverse) ## ── Attaching packages ──────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 3.1 Recap of day 2 3.1.1 Questions What is the tidyverse? What are the most important dplyr verbs and their function? Transfer: What do the following lines do? starwars %&gt;% group_by(homeworld) %&gt;% mutate(mass = mass / max(mass, na.rm = TRUE)) %&gt;% filter(homeworld == &quot;Tatooine&quot;) What is a p-value? What is the difference between binomially and hypergeometrically distributed data? 3.1.2 Answers 3.1.2.1 dplyr We use the starwars dataset to demonstrate the answers. select to select columns! select(starwars, name, height) filter to ask for specific rows based on a condition. filter(starwars, mass &gt; 100) mutate to add columns or modify existing columns. mutate(starwars, NewHeight = height * 2) Renaming columns works with rename but also inside of select. starwars %&gt;% rename(character = name) %&gt;% select(biggness = mass, character, height) %&gt;% head(1) ## # A tibble: 1 x 3 ## biggness character height ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 77 Luke Skywalker 172 arrange to sort based on a column. starwars %&gt;% arrange(mass) summarize to summarize columns down to a single value per column (note, that the tidyverse accepts American and British English, both functions exist). starwars %&gt;% summarize(max(height, na.rm = TRUE), min(mass, na.rm = TRUE)) ## # A tibble: 1 x 2 ## `max(height, na.rm = TRUE)` `min(mass, na.rm = TRUE)` ## &lt;int&gt; &lt;dbl&gt; ## 1 264 15 It is best to give names to the arguments, so we write it as: starwars %&gt;% summarize(MaxHeight = max(height, na.rm = TRUE), MinMass = min(mass, na.rm = TRUE)) ## # A tibble: 1 x 2 ## MaxHeight MinMass ## &lt;int&gt; &lt;dbl&gt; ## 1 264 15 group_by and summarise to summarise within each group in the grouping column(s): starwars %&gt;% group_by(homeworld) %&gt;% summarise(mass = mean(mass, na.rm = TRUE)) %&gt;% head(3) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 2 ## homeworld mass ## &lt;chr&gt; &lt;dbl&gt; ## 1 Alderaan 64 ## 2 Aleen Minor 15 ## 3 Bespin 79 Running the code in the first question thus demonstrates, how to normalize the mass to the maximum mass in each homeworld. For example, Luke Skywalker weights 56% as much as the heaviest person from his planet, while Darth Vader is the heaviest person on his planet (that made it into this dataset). starwars %&gt;% filter(!is.na(mass)) %&gt;% group_by(homeworld) %&gt;% mutate(mass = mass / max(mass) ) %&gt;% select(name, height, mass, homeworld) %&gt;% head(4) ## # A tibble: 4 x 4 ## name height mass homeworld ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalker 172 0.566 Tatooine ## 2 C-3PO 167 0.551 Tatooine ## 3 R2-D2 96 0.376 Naboo ## 4 Darth Vader 202 1 Tatooine 3.1.2.2 Statistics For the definition of p-values, check out chapter 2.6.2. The hypergeometric distribution is for sampling without replacement. x &lt;- rhyper(10000, m = 50, n = 50, k = 80) hist(x) The binomial distribution is for sampling with replacement (Like tossing a coin to your witcher). x &lt;- rbinom(n = 10000, size = 10, prob = 0.5) hist(x, breaks = 0:10) 3.2 Visualization with ggplot2 3.2.1 The Grammar of Graphics The idea of a “grammar of graphics” was first introduced by Leland Wilkinson (Wilkinson u. a. 2005). It enables us to describe graphics (data visualizations) in terms of relatively simple building blocks. Hadley Wickham took this idea and translated it with slight modifications into an R package (Wickham 2010). Understanding the individual building blocks and their grammar (how they connect) allows us to create intricate visualizations that can be iterated on with great speed without learning a different set of rules for each type of visualization. So, what makes up a graph? starwars %&gt;% filter(mass &lt; 500, !is.na(birth_year)) %&gt;% ggplot(aes(x = height, y = mass, color = gender)) + geom_point(aes(size = birth_year)) The building ground for our visualization is data. We then decide on some features in the data (like height, mass, gender and birth year) and map them to an aesthetic property. This is what happens inside the aes function as the mapping argument to ggplot. Height for example is mapped to the x-axis whereas gender is mapped to the color. “The color of what?”, you might ask, and rightfully so. There is no visualization without geometric objects to represent those aesthetics. This is where the functions starting with geom_ come into play. We add them to our plot with the + operator. Here, we use geom_point() to add points to our plot. This is a good fit because one point can represent several dimensions of our data at the same time (like, height as the x position, mass as the y position etc.). Other pieces of the grammar of graphics are not handled explicitly in the code above because the defaults work just fine. But they are handled nonetheless in the background: Scales define how exactly the data is mapped to aesthetics and can be modified with functions starting with scale_ (like scale_color_... to change the colors used in the color aesthetic, or scale_x_log10() to apply a logarithmic scale to the x-axis). Lastly, the objects with their scales need to placed on a coordinate system, like the default cartesian coordinates in the above example. Data =&gt; Aesthetic Mapping =&gt; Scales =&gt; (Statistical Transformation) =&gt; Geometric Objects =&gt; Coordinate System =&gt; Instead of displaying the plot straight away, we can save it to a variable: plt1 &lt;- starwars %&gt;% filter(mass &lt; 500, !is.na(birth_year)) %&gt;% ggplot(aes(x = height, y = mass, color = gender, label = name)) + geom_point(aes(size = birth_year)) And then display it later plt1 # not run Or pass it to other functions, like the handy ggplotly function from the plotly package, that creates and interactive java script visualization from a normal ggplot object: plotly::ggplotly(plt1) We can also use this object to pass it to the ggsave function to, well…, save it to a file. ggsave(&quot;myGGplot.png&quot;, plt2) A cool little R-package that comes with a RStudio addin is the colourpicker. This makes it easier to choose the colors for our plot. plt1 + scale_color_manual(values = c(&quot;#1f78b4&quot;, &quot;#D10808&quot;, &quot;#139E99&quot;, &quot;#59807A&quot;)) starwars %&gt;% count(gender) %&gt;% ggplot(aes(gender, n, fill = gender)) + geom_col() A shortcut for this is geom_bar() (it does the counting for us, similar to geom_histogram()). starwars %&gt;% ggplot(aes(gender, fill = gender)) + geom_bar() We can also pass different data to individual geometric layers e.g. to highlight certain parts in our data. The other geoms inherit the data passed into ggplot. The same holds true for aesthetics. aes defined in ggplot() are passed on to all geoms, but individual geoms can have their own aes, adding or overwriting the global aesthetics of the whole plot. justLuke &lt;- starwars %&gt;% filter(name == &quot;Luke Skywalker&quot;) plt2 &lt;- starwars %&gt;% filter(mass &lt; 500, birth_year &lt; 500) %&gt;% ggplot(aes(birth_year, mass)) + geom_point() + geom_line() + labs(x = &quot;birth year&quot;, y = &quot;mass [kg]&quot;, title = &quot;Awesome Plot 1&quot;, caption = &quot;Data from Starwars&quot;, subtitle = &quot;This is my subtitle&quot;, tag = &quot;A&quot;) + geom_point(data = justLuke, color = &quot;red&quot;) + geom_text(data = justLuke, aes(label = name), vjust = 1, hjust = 0) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;)) plt2 While saving this plot, we can also decide on the dimensions (in inch), pixel density (dots per inch, dpi) and other properties of the plot: ggsave(filename = &quot;plots/myFirstGggplot.png&quot;, plt2, width = 12, height = 12) Inside of rmarkdown documents, this is handled by the chunk options. E.g. {r, fig.width=5} leads to a differently sized plot. plt2 3.3 Visualization is Key Because visualization is so important, we dive into one example here: 3.3.1 Anscombes Quartet And a note on Tidy Data and the tidyr package. head(anscombe) ## # A tibble: 6 x 8 ## x1 x2 x3 x4 y1 y2 y3 y4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.7 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.1 8.84 7.04 Anscombes quartet is special because the datasets 1 to 4 look quite different but have the same x-mean, y-mean and simple linear regression (down to a couple of decimal digits)! paste( mean(anscombe$x1), mean(anscombe$x2), sd(anscombe$x1), sd(anscombe$x2) ) ## [1] &quot;9 9 3.3166247903554 3.3166247903554&quot; A cool trick from functional programming: Higher order functions. Higher order functions are functions that take other functions as arguments. They enable us to express ideas and instructions to the computer in a concise yet powerful manner. E.g. the map-family. Map functions apply a function to every element of a vector or list. addOne &lt;- function(x) x + 1 map(list(1,2,3), addOne) ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 3 ## ## [[3]] ## [1] 4 For all the datatypes (e.g. integer or double) there is a special version of map named map_&lt;datatype&gt;, which will always return this type and is thus safer to program with. map(anscombe, mean) ## $x1 ## [1] 9 ## ## $x2 ## [1] 9 ## ## $x3 ## [1] 9 ## ## $x4 ## [1] 9 ## ## $y1 ## [1] 7.500909 ## ## $y2 ## [1] 7.500909 ## ## $y3 ## [1] 7.5 ## ## $y4 ## [1] 7.500909 map_dbl(anscombe, mean) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 9.000000 9.000000 9.000000 9.000000 7.500909 7.500909 7.500000 7.500909 map_dbl(anscombe, sd) ## x1 x2 x3 x4 y1 y2 y3 y4 ## 3.316625 3.316625 3.316625 3.316625 2.031568 2.031657 2.030424 2.030579 3.3.2 Excursion Tidy Data Tidy Data means: Every row is an observation, every column is a feature. Figure 3.1: Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. (Source: R4DS, Hadley Wickham) But this is not always straightforward because… “Happy families are all alike; every unhappy family is unhappy in its own way.” \\(-\\) Leo Tolstoy Tidy datasets are all alike, but every messy dataset is messy in its own way.\" \\(-\\) Hadley Wickham 3.3.2.1 Anscombe The anscombe dataset actually hides a feature in its column names! The tidyr package helps us, to extract it into it’s own column. anscombe %&gt;% head(3) ## # A tibble: 3 x 8 ## x1 x2 x3 x4 y1 y2 y3 y4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.7 7.71 The feature hidden in the column names is the dataset (1 to 4). Belonging to a dataset is actually a property of each value. anscombe_long &lt;- anscombe %&gt;% pivot_longer(everything(), names_pattern = &quot;(.)(.)&quot;, names_to = c(&quot;.value&quot;, &quot;set&quot;) ) Now, each row represents one point (one observational unit), while every column represents a property / feature of said points. head(anscombe_long) ## # A tibble: 6 x 3 ## set x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 8.04 ## 2 2 10 9.14 ## 3 3 10 7.46 ## 4 4 8 6.58 ## 5 1 8 6.95 ## 6 2 8 8.14 Now our dataset plays better with the rest of the tidyverse, especially ggplot: anscombe_long %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ set) anscombe_long %&gt;% group_by(set) %&gt;% summarise(mean_x = mean(x), mean_y = mean(y) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 3 ## set mean_x mean_y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.50 ## 2 2 9 7.50 ## 3 3 9 7.5 ## 4 4 9 7.50 Or, to showcase a more functional style of programming: anscombe_long %&gt;% group_by(set) %&gt;% summarise_at(vars(x,y), list(mean = mean, sd = sd)) %&gt;% mutate_at(vars(-set), round, 2) ## # A tibble: 4 x 5 ## set x_mean y_mean x_sd y_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.5 3.32 2.03 ## 2 2 9 7.5 3.32 2.03 ## 3 3 9 7.5 3.32 2.03 ## 4 4 9 7.5 3.32 2.03 3.3.2.2 Plate Reader Example Figure 3.2: (Quelle: https://www.stellarscientific.com/accuris-smartreader-96-microplate-absorbance-plate-reader/ Sometimes, we end up with variable- / column-names that are not readily allowed. “Forbidden” names only work when they are encompassed by backticks (`). Thus, wrapping a variable name in backticks allows us to even have spaces and symbols in the name. But try to avoid this if possible! You will nevertheless encounter forbidden variable names when you read in data from a spreadsheet that has those names for its columns. Use rename to assign a better name or install the janitor package for the handy function janitor::clean_names(). rawData &lt;- tibble( `this is a &quot;forbidden&quot; name! #` = 1:5, `column 2` = rep(&quot;hi&quot;, 5) ) rawData ## # A tibble: 5 x 2 ## `this is a &quot;forbidden&quot; name! #` `column 2` ## &lt;int&gt; &lt;chr&gt; ## 1 1 hi ## 2 2 hi ## 3 3 hi ## 4 4 hi ## 5 5 hi Turns into rawData %&gt;% janitor::clean_names() ## # A tibble: 5 x 2 ## this_is_a_forbidden_name_number column_2 ## &lt;int&gt; &lt;chr&gt; ## 1 1 hi ## 2 2 hi ## 3 3 hi ## 4 4 hi ## 5 5 hi Reading in the example data from a plate reader, we end up with: raw_atpase &lt;- readxl::read_xlsx(&quot;data/04_ATPase_assay.xlsx&quot;, skip = 10) %&gt;% janitor::clean_names() head(raw_atpase[,1:6]) ## # A tibble: 6 x 6 ## content time_s sample_x1 sample_x2 sample_x3 sample_x4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raw Data (340) 0 0.737 1.33 1.71 1.81 ## 2 Raw Data (340) 60 0.789 1.35 1.65 1.71 ## 3 Raw Data (340) 120 0.656 1.29 1.66 1.70 ## 4 Raw Data (340) 180 0.531 1.23 1.66 1.70 ## 5 Raw Data (340) 240 0.49 1.18 1.67 1.72 ## 6 Raw Data (340) 300 0.486 1.12 1.67 1.74 Note that if you do not have the janitor package, you can also use the .name_repair argument to read_xlsx to repair the names while reading the data, but the names produced in doing so are not as pretty. readxl::read_xlsx(&quot;data/04_ATPase_assay.xlsx&quot;, skip = 10, .name_repair = &quot;universal&quot;) %&gt;% head(1) ## # A tibble: 1 x 14 ## Content Time..s. Sample.X1 Sample.X2 Sample.X3 Sample.X4 Sample.X5 Sample.X6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raw Da… 0 0.737 1.33 1.71 1.81 1.01 1.44 ## # … with 6 more variables: Sample.X7 &lt;dbl&gt;, Sample.X8 &lt;dbl&gt;, Sample.X9 &lt;dbl&gt;, ## # Sample.X10 &lt;dbl&gt;, Sample.X11 &lt;dbl&gt;, Sample.X12 &lt;dbl&gt; New names: * Time [s] -&gt; Time..s. * … Clean the data, make it tidy tidy_atpase &lt;- raw_atpase %&gt;% select(-content) %&gt;% pivot_longer( -time_s, names_to = &quot;sample&quot;, values_to = &quot;absorbance&quot; ) %&gt;% rename(time = time_s) Visualize data tidy_atpase %&gt;% ggplot(aes(time, absorbance, color = sample)) + geom_line() Normalize data normalized_atpase &lt;- tidy_atpase %&gt;% group_by(sample) %&gt;% mutate(absorbance = absorbance / max(absorbance)) %&gt;% ungroup() normalized_atpase %&gt;% ggplot(aes(time, absorbance, color = sample)) + geom_point() + geom_line() 3.3.3 Summary Statistics 3.3.3.1 Standard Deviation \\[sd = \\sqrt{\\frac{\\sum_{i=0}^{n}{(x_i-\\bar x)^2}}{(n-1)} }\\] Why n-1? The degrees of freedom are n reduced by 1 because if we know the mean of a sample, once we know all but 1 of the individual values, the last value is automatically known and thus doesn’t count towards the degrees of freedom. 3.3.3.2 Variance The variance is the squared standard deviation. \\[var = \\sigma^2\\] 3.3.3.3 Standard Error of the Mean Often called SEM or SE. \\[SEM=\\sigma / \\sqrt{n}\\] 3.4 Distributions x &lt;- rnorm(1000) SD &lt;- sd(x) hist(x, probability = TRUE) curve(dnorm, add = TRUE, col = &quot;red&quot;) abline(v = SD, col = &quot;red&quot;) abline(v = -SD, col = &quot;red&quot;) SEM sd(x) / sqrt(length(x)) ## [1] 0.03077073 3.4.1 From Distributions to Quantiles d stands for density i.e. the probability density function of a distribution. curve(dnorm, -4, 4) Figure 3.3: Probability density function for the normal distribution. p stands for probability. And it represents the cumulative probability i.e. the integral of the density function from \\(-\\infty\\) to \\(x\\). curve(pnorm, -4, 4) Figure 3.4: pnorm of x is the probability to draw a value less than or equal than x from a normal distribution. q stands for quantile. It is the inverse of the probability density function (so the axis are swapped). curve(qnorm) Quantiles! 3.4.1.1 Quantile-Quantile Plots Quantile-Quantile plots can answer the question “Does my data follow a certain distribution?” In this case: “Is my data normally distributed?” qqnorm(x) qqline(x, col = &quot;red&quot;) 3.5 The Datasaurus Dozen The Datasaurus Dozen is a great dataset, showcasing similar properties to anscombes quartet (but in a more impressive way). datasauRus::datasaurus_dozen %&gt;% ggplot(aes(x,y)) + geom_point(size = 0.9) + facet_wrap(~dataset) + coord_equal() + theme_classic() You can find the research paper here (matejka2017a?). But not only does it highlight the importance of not relying solely on summary statistics, it also comes with additional datasets. One of them is designed to show the potential problems of so called box-plots: dinos &lt;- datasauRus::box_plots head(dinos) ## # A tibble: 6 x 5 ## left lines normal right split ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -9.77 -9.77 -9.76 -9.76 -9.77 ## 2 -9.76 -9.74 -9.72 -9.05 -9.77 ## 3 -9.75 -9.77 -9.68 -8.51 -9.77 ## 4 -9.77 -9.77 -9.64 -8.24 -9.77 ## 5 -9.76 -9.77 -9.6 -8.82 -9.77 ## 6 -9.77 -9.76 -9.56 -8.07 -9.76 This data is not in the tidy format but this can easily be fixed: tidy_dinos &lt;- dinos %&gt;% pivot_longer(everything(), names_to = &quot;set&quot;, values_to = &quot;value&quot;) head(tidy_dinos) ## # A tibble: 6 x 2 ## set value ## &lt;chr&gt; &lt;dbl&gt; ## 1 left -9.77 ## 2 lines -9.77 ## 3 normal -9.76 ## 4 right -9.76 ## 5 split -9.77 ## 6 left -9.76 Boxplots! tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_boxplot() A Boxplot shows the median (50th percentile) as a black line, the 25th and 75th percentile (= first and third quartile) as lower and upper limits of the box, as well as whiskers. The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). […] Data beyond the end of the whiskers are called “outlying” points and are plotted individually (from ?geom_boxplot). However, this type of plot ignores the underlying distribution of datapoints. And if we look at the actual points, the full range of what the boxplot didn’t tell us can be seen: tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_boxplot() + geom_jitter(alpha = 0.1) tidy_dinos %&gt;% group_by(dataset = set) %&gt;% summarise(Median = median(value) %&gt;% round(2)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## dataset Median ## &lt;chr&gt; &lt;dbl&gt; ## 1 left -0.01 ## 2 lines -0.01 ## 3 normal 0 ## 4 right 0 ## 5 split 0 Plotting the points individually with geom_point doesn’t quite work here because they overlap too much. geom_jitter helps us out by distributing the points an the x-axis. We are allowed to move the points on the x-axis a bit because it is a categorical axis, not a continuous. Shifting the points within their categories does not change the data. We could not have moved the points on the y-axis because that would have falsified the data. tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_point(alpha = 0.01) tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_jitter() An even more problematic visualization would have been to display the data with a barplot of means and error bars for the SD or SEM. Friends don’t let friends make barplots! Histograms or density plots on the other hand tell us about the distribution, not just summary statistics, while at the same time not overloading the plot like the points do in some cases. tidy_dinos %&gt;% ggplot(aes(value, fill = set)) + geom_histogram(bins = 50) + facet_wrap(~ set) tidy_dinos %&gt;% ggplot(aes(value, fill = set)) + geom_density() + facet_wrap(~ set) The ggbeeswarm-package gives us even more possibilities to display all those points in an orderly fashion. tidy_dinos %&gt;% ggplot(aes(set, value, fill = set)) + ggbeeswarm::geom_quasirandom(method = &quot;smiley&quot;) 3.6 Exercises 3.6.1 Inclusion Bodies You can get the dataset from my Github Repository. Disclaimer: This is real biological data but the explanation and context are changed. If you want to impress me, do all the exercises in an Rmarkdown document, add your conclusions and thoughts along with the data analysis process and structure it with meaningful headlines using #. Read the csv-file data/03_inclusion_bodies.csv. Make it tidy Visualize the data with ggplot as: Jittered point plot Boxplot Two overlaid histograms (Hint: use position = \"identity\" so that R doesn’t stack the bars) Two overlaid density plots (Hint: use the parameter alpha to make both of them visible at the same time) A Barplot with the mean and error bars, hints: Create a summary tibble first Use geom_col, not geom_bar BONUS: Make the plots pretty with ggplot theme options 3.6.2 Solutions From the last course: Rmarkdown document Result as html document Result as pdf document "],
["day4.html", "Day 4 Significance Tests and Power 4.1 The Central Limit Theorem (CLT) 4.2 The T-Distribution 4.3 Significance Tests 4.4 Type I and Type II Errors 4.5 Problems 4.6 Exercises", " Day 4 Significance Tests and Power 4.1 The Central Limit Theorem (CLT) From distributions of measurements to distributions of means. What if Johanna didn’t count as many cells? Say 30 instead of 300? To turn this into a demonstration, we assume that the 300 cells are completely representative of all WT and KO cells and thus call this our population. Now we draw a sample of 30 from the population We calculate the mean And repeat this procedure 1000 times What does the distribution of means look like? 4.1.1 Reading in Data Firstly, we load the tidyverse and our data. library(tidyverse) ## ── Attaching packages ──────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # import data for inclusion bodies data &lt;- read_csv(&quot;data/03_inclusion_bodies.csv&quot;) ## Parsed with column specification: ## cols( ## wt = col_double(), ## ko = col_double() ## ) # attach data, handle with CARE! attach(data) attach makes the columns of a data sets (in our case ko and wt) globally available as vectors. This enables us to write wt instead of data$wt or data[\"wt\"]. Use attach carefully and only in very few situations! The order of those two vectors is now independent of each other. Operations on the vectors wt and ko do not influence the other vector or the original column of the data frame data! 4.1.2 Programming and Problem Solving This approach will help you not only for this exercise but for all sorts of problems that come up in programming. First, find the smallest piece of the problem that we want to solve. 4.1.2.1 The basic Case This is exactly the first part of our road map: Now we draw a sample of 30 from the population We calculate the mean # sample n = 30 points from a vector draw &lt;- sample(wt, 30, replace = TRUE) # calculate mean mean(draw) ## [1] 3.633333 4.1.2.2 Abstraction for many Cases We do not want to type the above lines 1000 times. One way to solve this would be with a for-loop. But there is a more elegant way. This might seem daunting at first but will enable you to solve very complex problems faster in the long run. We turn the two lines of code above into a function. And while doing so also introduce a bit of abstraction (generalization) We create a function that takes a vector (x), draws n elements from it and returns the mean of those elements. # getMeanOfSubset getMeanOfSubset &lt;- function(x, n) { draw &lt;- sample(x, n, replace = TRUE) mean(draw) } Now we can now test that the function is working. # test getMeanOfSubset getMeanOfSubset(wt, 30) ## [1] 2.5 Remember the higher order functions introduced in the chapter about anscombes quartet (3.3.1)? We now use the map function from the purrr-package to call this function 1000 times and save the result in a list or vector (map_dbl for map double). # define N N &lt;- 1000 # map over 1:N wtMeans &lt;- map(1:N, ~ getMeanOfSubset(wt, 30)) Now you might be asking yourselves: “What is this weird tilde symbol doing there?” 4.1.2.3 Excursion: Lambda Functions \\[\\lambda\\] The ~ symbol (tilde) in the context of map functions creates a so called lambda function. Lambda functions are anonymous functions, functions without a name. So we generally use them whenever we need a function only once. function(x) x + 1 is equivalent to ~ .x + 1. The argument passed to the lambda function is always called .x inside of the function. So in map(1:N, ~ getMeanOfSubset(wt, 30)), the lambda function ~ getMeanOfSubset(wt, 30) takes the numbers from 1 to N (1:N) but completely ignores them (there is no mention of .x). The result is getMeanOfSubset being called 1000 times with the same arguments (wt and 30). We can explicitly request the results as a vector of numbers instead of a list with map_dbl. wtMeans &lt;- map_dbl(1:N, ~ getMeanOfSubset(wt, 30)) 4.1.3 Population Distribution vs. Distribution of Means With large enough n, the distribution of means follows a normal distribution, even though the values are not normally distributed. wtMeans &lt;- map_dbl(1:N, ~ getMeanOfSubset(wt, 30)) hist(wtMeans) hist(wt) According to the Central Limit Theorem the means follow a normal distribution. We can show this. qqnorm(wtMeans) qqline(wtMeans, col = &quot;red&quot;) For another great visualization of the central limit theorem, check out this interactive tutorial by Seeing Theory. 4.2 The T-Distribution The CLT is only valid for large sample sizes. For smaller sample sizes, the distribution of means has fatter tails than a normal distribution This is why for most statistical tests, we use the t-distribution instead of the normal distribution. For 3 degrees of freedom (DF): tdist &lt;- function(x) dt(x, df = 3) curve(dnorm, -3, 3) curve(tdist, -3, 3, add = TRUE, col = &quot;red&quot;) Figure 4.1: t-distribution in red, normal distribution in black. For 30 DF: tdist &lt;- function(x) dt(x, df = 30) curve(dnorm, -3, 3) curve(tdist, -3, 3, add = TRUE, col = &quot;red&quot;) Figure 4.2: t-distribution in red, normal distribution in black. 4.2.1 Confidence Intervals (CIs) We can use the t-distribution to calculate (95%-) confidence intervals. The 95% CI of a sample mean contains the true mean (the mean of the population) in 95% of cases (if you where to repeat the experiment infinitely often). hist(wtMeans) abline(v = mean(wt), col = &quot;red&quot;) Figure 4.3: True mean in red. draw &lt;- sample(wt, 7) CI &lt;- t.test(draw)$conf.int hist(draw) abline(v = CI[1], col = &quot;red&quot;) abline(v = CI[2], col = &quot;red&quot;) 4.3 Significance Tests Significance tests answer the question: “Under the assumption of no difference between two (or more) groups (i.e. they are samples of the same population), what is the probability to find a difference as large as or more extreme than the difference observed.” 4.3.1 Students T-Test For normally distributed data, we would use students t-test. t.test(wt, ko) ## ## Welch Two Sample t-test ## ## data: wt and ko ## t = -26.929, df = 476.21, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -13.19393 -11.39940 ## sample estimates: ## mean of x mean of y ## 3.043333 15.340000 wtDraw &lt;- sample(wt, 3, replace = TRUE) koDraw &lt;- sample(ko, 3, replace = TRUE) t.test(wtDraw, koDraw) ## ## Welch Two Sample t-test ## ## data: wtDraw and koDraw ## t = -2.2283, df = 3.0218, p-value = 0.1115 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.378879 3.378879 ## sample estimates: ## mean of x mean of y ## 5.666667 13.666667 But our data is not normally distributed! data %&gt;% pivot_longer(c(1,2)) %&gt;% ggplot(aes(name, value, fill = name)) + geom_jitter() 4.3.2 Wilcoxon Rank Sum Test With non-normal data, we need a so called non-parametric test. Those tests do not make the assumption of normality. The Wilcoxon Rank Sum test (also called Mann-Whitney U test) takes the values and converts them into “ranks” before comparing them. The rank of a value in our dataset answers the question: “When sorting the values from lowest to highest, what is the index of that value?” Example: x &lt;- c(1,2,1.12312, 4000000, 42) rank(x) ## [1] 1 3 2 5 4 We run this test in R as follows: wilcox.test(wt, ko) ## ## Wilcoxon rank sum test with continuity correction ## ## data: wt and ko ## W = 4707.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 4.4 Type I and Type II Errors Type I: False Positives (rejection of a true null hypothesis) Type II: False Negatives (non-rejection of false null hypothesis) 4.4.1 Type I: False Positives A type I error means we find a difference between samples even though there is none (they are from the same population) We often define a p-value \\(\\leq\\) 0.05 (= 5 %) as a “statistically significant” result. Keep in mind, that this cutoff is arbitrary and has no physical meaning. This cutoff also means that by definition we are accepting a minimum of 5% false positives! This cutoff, the significance level is also called \\(\\alpha\\). \\[\\alpha=\\text{Type I error rate}\\] curve(dnorm, -3, 3) n &lt;- 5 draw1 &lt;- rnorm(n) draw2 &lt;- rnorm(n) t.test(draw1, draw2) ## ## Welch Two Sample t-test ## ## data: draw1 and draw2 ## t = 0.85987, df = 7.9773, p-value = 0.415 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9470132 2.0723043 ## sample estimates: ## mean of x mean of y ## 0.571204782 0.008559254 wilcox.test(draw1, draw2) ## ## Wilcoxon rank sum exact test ## ## data: draw1 and draw2 ## W = 16, p-value = 0.5476 ## alternative hypothesis: true location shift is not equal to 0 Test this code with different values of n. 4.4.2 Type II: False Negatives If there exists a true difference between samples (i.e. they come from a different population), but we do not detect that difference (“not statistically significant”), we commit a type II error. A type II error is thus a false negative result. the type II error rate is called \\(\\beta\\) (analogous to \\(\\alpha\\)). \\[\\beta=\\text{Type II error rate}\\] n &lt;- 5 draw1 &lt;- rnorm(n, mean = 0) draw2 &lt;- rnorm(n, mean = 1) t.test(draw1, draw2) ## ## Welch Two Sample t-test ## ## data: draw1 and draw2 ## t = -0.93826, df = 7.8395, p-value = 0.3761 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.464869 1.042769 ## sample estimates: ## mean of x mean of y ## 0.5616486 1.2726987 wilcox.test(draw1, draw2) ## ## Wilcoxon rank sum exact test ## ## data: draw1 and draw2 ## W = 9, p-value = 0.5476 ## alternative hypothesis: true location shift is not equal to 0 Repeat with different values of n. 4.4.3 Statistical Power The power of a statistical test is the probability to classify a real difference as statistically significant (with the chosen \\(\\alpha\\)). So it represents true positives. \\[Power = 1-\\beta\\] In R, we have the function power.t.test, which can tell us the power (or any of the arguments left blank / set to NULL) power.t.test(n = NULL, delta = 5, sd = 1, sig.level = 0.05, power = 0.9) ## ## Two-sample t test power calculation ## ## n = 2.328877 ## delta = 5 ## sd = 1 ## sig.level = 0.05 ## power = 0.9 ## alternative = two.sided ## ## NOTE: n is number in *each* group 4.5 Problems 4.5.1 The Jelly Bean Problem (Multiple Testing) Controlling the False Discovery Rate (FDA): 4.5.1.1 Bonferroni correction pValues &lt;- c(0.5, 0.05, 0.3, 0.001, 0.003) p.adjust(pValues, method = &quot;bonferroni&quot;) ## [1] 1.000 0.250 1.000 0.005 0.015 4.5.1.2 Benjamini-Hochberg procedure Sort all p-values in ascending order Choose a FDR \\(q\\) and call the number of tests done \\(m\\) Find the largest p-value with: \\(p \\leq iq/m\\) with its index \\(i\\). This is your new threshold for significance p.adjust(pValues, method = &quot;BH&quot;) ## [1] 0.50000000 0.08333333 0.37500000 0.00500000 0.00750000 4.5.2 The Base Rate Fallacy Example: mammogram Sensitivity = Power = true positive rate Specificity = true negative rate = \\(1-\\alpha\\) total &lt;- 1000 positives &lt;- 10 negatives &lt;- total - positives sensitivity &lt;- 0.9 specificity &lt;- 1 - 0.08 true_positives &lt;- sensitivity * positives false_positives &lt;- (1 - specificity) * negatives p_positive &lt;- true_positives / (true_positives + false_positives) p_positive ## [1] 0.1020408 Bayes Formula: \\[P(A|B)=\\frac{P(B|A)*P(A)}{P(B)}\\] With \\(P(X)\\) as the probability of \\(X\\) and \\(P(X|Y)\\) being the probability of \\(X\\) given \\(Y\\) (conditional probability). In terms of our example: \\[P( Cancer | positive Test)=\\frac{P(positive Test|Cancer)*P(Cancer)}{P(positive Test)}\\] The probability of having the disease given a positive test result (\\(P(A|B)\\)), is equal to the probability of testing positive while having the disease (the sensitivity!) times the probability of having the disease just based on the prevalence of the disease (this is also called the prior probability, \\(P(A)\\)) divided by the probability of testing positive, no matter the condition. The latter part, the denominator, can be broken down further to be expressed in terms of our definitions from the beginning of this section: \\[P( Cancer | positive Test)=\\frac{P(positive Test|Cancer)*P(Cancer)}{P(positive Test | Cancer) * P(Cancer) + P(positive Test | no Cancer) * P(no Cancer)}\\] So in other words: \\[P( Cancer | positive~test)=\\frac{Sensitivity*Prevalence}{Sensitivity * Prevalence + (1-Specificity) * (1-Prevalence)}\\] 4.5.3 Resources Statistics Done Wrong: https://www.statisticsdonewrong.com/ Intuitive Biostatistics, also in the Uni-Bib: https://katalog.ub.uni-heidelberg.de/cgi-bin/titel.cgi?katkey=68260114&amp;sess=050a1316767b181982c9bce94283e9ae&amp;query=Intuitive%20Biostatistics https://www.graphpad.com/guides/prism/8/statistics/index.htm 4.6 Exercises 4.6.1 Show the following statements with simulations in R 4.6.1.1 Increasing the sample size n reduces the standard error of the mean with the square root of n. Draw 10 cells from the wt (or ko) vector calculate the mean write a function that does that run it 1000 times and save the result in a vector calculate the SD of the means now, draw 40 instead of 10 cells and repeat. How does the SD change? Write a function of n that does above steps Feed the numbers from 1 to 100 to the function and plot the resulting SDs getMeanOfSample &lt;- function(x, n) sample(x, n, replace = TRUE) %&gt;% mean() getSdOfManyMeans &lt;- function(n, x) { map_dbl(1:100, ~ getMeanOfSample(x, n) ) %&gt;% sd() } sdByN &lt;- tibble(N = 1:100) %&gt;% mutate(sd = map_dbl(N, getSdOfManyMeans, x = wt)) sdByN %&gt;% ggplot(aes(N, sd)) + geom_point() 4.6.1.2 A 95% confidence interval of a sample contains the true mean (of the population) with a probability of 95% draw 30 cells from the wt (or ko) vector Calculate the limits of the CI write a function that does that use map or replicate to get 1000 sets of limits Write a function that tests, if a set of limits contains the true mean Map this function over the list of limits How often to you obtain TRUE? calculateCIforSample &lt;- function(x, n) { sample(x, n, replace = TRUE) %&gt;% t.test() %&gt;% pluck(&quot;conf.int&quot;) } testLimitContainsMean &lt;- function(limit, M) { limit[1] &lt;= M &amp;&amp; limit[2] &gt;= M } manyCIs &lt;- map(1:100, ~ calculateCIforSample(wt, 30)) map_lgl(manyCIs, testLimitContainsMean, M = mean(wt)) %&gt;% mean() ## [1] 0.91 Optionally manyCIs %&gt;% enframe() %&gt;% unnest_wider(value) %&gt;% head() ## # A tibble: 6 x 3 ## name ...1 ...2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.10 2.77 ## 2 2 1.47 4.27 ## 3 3 1.59 4.88 ## 4 4 1.24 3.16 ## 5 5 2.00 5.20 ## 6 6 2.74 6.32 4.6.2 Show the following concepts with simulations in R 4.6.2.1 Sensitivity: With a true difference existing, how large is the probability to detect it (i.e. get a p-value &lt;= 0.05) with a wilcoxon rank sum test? Draw a sample of 30 from the wt and the ko vector and test for a statistically significant difference Do this 1000 times. How many are statistically significant? What changes when you draw 10 instead of 30? Or 5? runWilcoxOnSample &lt;- function(n, x1, x2) { draw1 &lt;- sample(x1, n, replace = TRUE) draw2 &lt;- sample(x2, n, replace = TRUE) wilcox.test(draw1, draw2, exact = FALSE)$p.value } pValues &lt;- map_dbl(1:1000, ~ runWilcoxOnSample(9, wt, ko)) mean(pValues &lt;= 0.05) ## [1] 0.993 hist(pValues) ?wilcox.test 4.6.2.2 Specificity: With no difference existing, what is the probability to detect one nonetheless? Imagine all cells are wt-cells Draw two samples of n = 30 from the wt-cells (i.e. the same population) and run a wilcoxon rank sum test Repeat 1000 times How are the p-values distributed? How often is the result statistically significant? pValues &lt;- map_dbl(1:1000, ~ runWilcoxOnSample(10, wt, wt)) mean(pValues &lt;= 0.05) ## [1] 0.049 hist(pValues) 4.6.2.3 Resource for p-value Histograms http://varianceexplained.org/statistics/interpreting-pvalue-histogram/ 4.6.2.4 Further notes source(&quot;usefullFunctions.R&quot;) Note: I showed you a lot of map functions, but often, you don’t need a map function because a lot of the basic operations in R are vectorised by default (so they operate on elements of vectors). With two vectors x and y x &lt;- 1:5 y &lt;- 1:5 We can just write x + y ## [1] 2 4 6 8 10 Instead of map2_dbl(x,y, `+`) ## [1] 2 4 6 8 10 "],
["day5.html", "Day 5 Correlation and Regression 5.1 Side note: What is the main goal of this course? 5.2 Correlation and Regression 5.3 Non-Linear Regression 5.4 Exercises", " Day 5 Correlation and Regression 5.1 Side note: What is the main goal of this course? I can not teach you every single function, every data structure, every statistical analysis. Rather, I want to give you a high level overview of what is possible. And I want to teach you how you can view your data problems in a way that enables you to solve them. We want to go from seeing the numbers To seeing the structure behind it 5.2 Correlation and Regression The data for today are available from their original source or my github repository. It’s about storks! And because according to folk wisdom storks bring the babies, we get the number of stork pairs per country and the birthrate in \\(10^3\\) babies per year. Additionally, we get the number of people per country and the area in \\(km^2\\). library(tidyverse) ## ── Attaching packages ──────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() storks &lt;- read_csv(&quot;data/05_storks.csv&quot;) ## Parsed with column specification: ## cols( ## Country = col_character(), ## Area = col_double(), ## Storks = col_double(), ## Humans = col_double(), ## Birth = col_double() ## ) head(storks) ## # A tibble: 6 x 5 ## Country Area Storks Humans Birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 28750 100 3.2 83 ## 2 Austria 83860 300 7.6 87 ## 3 Belgium 30520 1 9.9 118 ## 4 Bulgaria 111000 5000 9 117 ## 5 Denmark 43100 9 5.1 59 ## 6 France 544000 140 56 774 This data shows a clear relationship between the number of stork pairs and the birthrate in a country (matthews2000?). # show storks vs birth rate with ggplot, points ggplot(storks, aes(Storks, Birth)) + geom_point() The data looks better on a logarithmic scale, this diminishes the effect of outliers in our plot. # ggplot with dual log scale and annotation_logticks, theme classic ggplot(storks, aes(Storks, Birth)) + geom_point() + scale_x_log10() + scale_y_log10() + annotation_logticks() + theme_classic() Log transformations are in fact quite important for a range of applications. Especially in biological data, you will encounter them quite a bit. The reason is as follows: While the central limit theorem states that the sum of independent random variables tends towards a normal distribution, biological processes often contain signal cascades and other multiplicative effects. A logarithm conveniently turns a product into a sum, so while your original data may not look very much like a normal distribution, it might very well be log-normal. 5.2.1 Correlation We get a measure of linear relation between to random variables with the (Pearson’s) correlation coefficient. \\[cor(X,Y)=\\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}=\\frac{E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]}{\\sigma_X \\sigma_Y}\\] \\(cor\\) is equal to the quotient of the co variance \\(cov\\) and the product of the standard deviations \\(\\sigma\\). The co variance in turn is the expected value (i.e. the mean) of the element-wise product of the difference of vector X to its mean and the difference of vector Y to its mean. cor(storks$Storks, storks$Birth) ## [1] 0.6202653 cor(log(storks$Storks), log(storks$Birth)) ## [1] 0.3853895 The correlation coefficient is also called Pearson’s R, and its square consequently is R squared. This value is the coefficient of determination, the fraction of explained variance of one variable by the other. cor(storks$Storks, storks$Birth)^2 ## [1] 0.3847291 Both cor and cor.test can used the Spearman method to return correlation of ranks instead of values. cor(storks$Storks, storks$Birth, method = &quot;spearman&quot;) ## [1] 0.4176917 cor(log(storks$Storks), log(storks$Birth), method = &quot;spearman&quot;) ## [1] 0.4176917 Figure 5.1: Source: XKCD, https://www.xkcd.com/552/ 5.2.2 Regression A related concept is linear regression. model &lt;- lm(Birth ~ Storks, data = storks) summary(model) ## ## Call: ## lm(formula = Birth ~ Storks, data = storks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -478.8 -166.3 -144.9 -2.0 631.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.250e+02 9.356e+01 2.405 0.0295 * ## Storks 2.879e-02 9.402e-03 3.063 0.0079 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 332.2 on 15 degrees of freedom ## Multiple R-squared: 0.3847, Adjusted R-squared: 0.3437 ## F-statistic: 9.38 on 1 and 15 DF, p-value: 0.007898 This R squared value is in fact the coefficient of determination that you saw earlier for correlation. When your linear model is free to choose an intercept, this R squared also has the same value as for correlation. Using geom_smooth, we can fit any model to our data right within ggplot, even after applying the log-transformation. storks %&gt;% ggplot(aes(Storks, Birth)) + geom_point() + scale_x_log10() + scale_y_log10() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; But what happens when our data does not follow a linear model? 5.3 Non-Linear Regression 5.3.1 Michaelis-Menten Kinetics As an example we will use the common Michaelis-Menten kinetics. There is a dataset for enzyme reaction rates included in R. puromycin &lt;- as_tibble(Puromycin) treatedPuro &lt;- puromycin %&gt;% filter(state == &quot;treated&quot;) head(treatedPuro) ## # A tibble: 6 x 3 ## conc rate state ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.02 76 treated ## 2 0.02 47 treated ## 3 0.06 97 treated ## 4 0.06 107 treated ## 5 0.11 123 treated ## 6 0.11 139 treated plot(treatedPuro$conc, treatedPuro$rate, main = &quot;In case you miss base-R&quot;) treatedPuro %&gt;% ggplot(aes(conc, rate)) + geom_point() + theme_classic() This data follows a Michaelis-Menten kinetic, so we define a function that translates a concentration into a rate given a maximal velocity Vm and the Michaelis-Menten constant K. Having the concentration as the first argument helps for later usage of the function. calcMicMen &lt;- function(conc, Vm, K) { (Vm * conc) / (K + conc) } calcMicMen is actually vectorised by default, because it just used basic mathematical operations: x &lt;- seq(0, 1, by = 0.01) y &lt;- calcMicMen(x, Vm = 200, K = 0.1) plot(x,y) We can plot this function with some initial values for Vm and K using stat_function (it is a stat, not a geom, because it needs to do some calculation on the data before plotting it). Note that we use a lambda function to define a function on the fly that is only dependent on one argument, passed as .x and already has Vm and K set at some value we chose. treatedPuro %&gt;% ggplot(aes(conc, rate)) + geom_point() + stat_function(fun = ~ calcMicMen(conc = .x, Vm = 200, K = 0.1), color = &quot;red&quot;) Another way to write this is using the args argument of stat_fun. This way, we can supply just the name of the function and additional arguments such as Vm and K go into a list passed to args. The only argument we do not supply (conc) is then automatically taken from the x-axis and the result uses as the y-value. # output not shown because it is the same as above treatedPuro %&gt;% ggplot(aes(conc, rate)) + geom_point() + stat_function(fun = calcMicMen, args = list(Vm = 200, K = 0.1), color = &quot;red&quot;) But this function doesn’t fit our data! It just has some Vm and K that we manually put in, so let’s use R to move it closer to our data. This process of taking some initial function and moving it closer to our data is called least squares regression and as opposed to the simple case of linear models earlier, here we are using a non-linear model and thus the function nls for non-linear least squares. model &lt;- nls(formula = rate ~ calcMicMen(conc, Vm, K), data = treatedPuro, start = list(Vm = 200, K = 0.1)) model ## Nonlinear regression model ## model: rate ~ calcMicMen(conc, Vm, K) ## data: treatedPuro ## Vm K ## 212.68363 0.06412 ## residual sum-of-squares: 1195 ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 6.093e-06 It also tells us the residual sum of squares, which is a measure of distance between the function and our data. For a brilliant interactive visualization of this process, check out this link (O A)! summary(model) ## ## Formula: rate ~ calcMicMen(conc, Vm, K) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Vm 2.127e+02 6.947e+00 30.615 3.24e-11 *** ## K 6.412e-02 8.281e-03 7.743 1.57e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.93 on 10 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 6.093e-06 We get the estimated coefficients (the fitted Vm and K) from our model with the function coef. Vm_est &lt;- coef(model)[&quot;Vm&quot;] K_est &lt;- coef(model)[&quot;K&quot;] coef(model) ## Vm K ## 212.68362994 0.06412111 Now we can supply those as arguments to the calcMicMen function used in stat_function treatedPuro %&gt;% ggplot(aes(conc, rate)) + geom_point() + stat_function(fun = calcMicMen, args = list(Vm = Vm_est, K = K_est), color = &quot;red&quot;) Side note: There is also self starting models, but I do not cover them a lot because their use case is more specific than being able to fit any self defined function. Self starting models are able to estimate sensible starting parameters from the data. nls(formula = rate ~ SSmicmen(conc, Vm, K), data = treatedPuro) ## Nonlinear regression model ## model: rate ~ SSmicmen(conc, Vm, K) ## data: treatedPuro ## Vm K ## 212.68371 0.06412 ## residual sum-of-squares: 1195 ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 1.929e-06 You can use any model created with a modeling function such as nls or lm with the predict function to predict new values for new data: predict(model, newdata = list(conc = c(0, 0.5, 1))) ## [1] 0.0000 188.5088 199.8679 5.3.2 Many Models The superpower of tibbles! Our data set actually contained two experimental conditions, one where the enzyme was treated and another where it was not. puromycin %&gt;% ggplot(aes(conc, rate, color = state)) + geom_point() We want to fit models to both conditions individually, without having to write the same code twice. In order to achieve this, we first nest the data. We retain one column with the state and the data for that state is in another column called data and stored as a list of tibbles (one tibble for each state). So we go from this: head(puromycin) ## # A tibble: 6 x 3 ## conc rate state ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.02 76 treated ## 2 0.02 47 treated ## 3 0.06 97 treated ## 4 0.06 107 treated ## 5 0.11 123 treated ## 6 0.11 139 treated To this: nestedPuromycin &lt;- puromycin %&gt;% group_nest(state) nestedPuromycin %&gt;% head() ## # A tibble: 2 x 2 ## state data ## &lt;fct&gt; &lt;list&lt;tbl_df[,2]&gt;&gt; ## 1 treated [12 × 2] ## 2 untreated [11 × 2] Just to check that we didn’t lose anything, here I pluck the data column, from this I pluck the first element and then I take only the first 5 rows of this element with head. nestedPuromycin %&gt;% pluck(&quot;data&quot;, 1) %&gt;% head(5) ## # A tibble: 5 x 2 ## conc rate ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.02 76 ## 2 0.02 47 ## 3 0.06 97 ## 4 0.06 107 ## 5 0.11 123 Now we go on fitting a model for every element of the data column, so for every sub-dataset (one for each state). I also apply the coef function to all resulting models and go straight ahead with unnesting those coefficients into their own columns. nestedPuromycin %&gt;% mutate( model = map(data, ~ nls(formula = rate ~ calcMicMen(conc, Vm, K), data = .x, start = list(Vm = 200, K = 0.1))), coefficients = map(model, coef) ) %&gt;% unnest_wider(coefficients) %&gt;% head() ## # A tibble: 2 x 5 ## state data model Vm K ## &lt;fct&gt; &lt;list&lt;tbl_df[,2]&gt;&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 treated [12 × 2] &lt;nls&gt; 213. 0.0641 ## 2 untreated [11 × 2] &lt;nls&gt; 160. 0.0477 ggplot is also able to fit individual models and it can use any fitting-function in geom_smooth, not just linear models. So nothing is stopping us from writing: puromycin %&gt;% ggplot(aes(conc, rate, color = state)) + geom_point() + geom_smooth( method = &quot;nls&quot;, formula = y ~ calcMicMen(conc = x, Vm, K), method.args = list(start = list(Vm = 200, K = 0.1)), se = FALSE ) Note that we need to supply the formula for the method nls in a general form (with y ~ x) and the additionally arguments that nls needs, namely the starting parameters, are supplied as a list with method.args. We also set the standard error se to FALSE because \"nls\" doesn’t report it, so if we where to try and display it, we would get an error message. 5.4 Exercises 5.4.1 With the Datasaurus Dozen Data Sets datasauRus::datasaurus_dozen Visualize all data sets in one ggplot, Hint: usefacet_wrap Add linear regression lines Hint: use geom_smooth Do the fit manually on the dataset and extract the coefficients Hints: See above, nest the data first, create the model column, etc. 5.4.1.1 Solutions dinos &lt;- datasauRus::datasaurus_dozen head(dinos) ## # A tibble: 6 x 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 dinos %&gt;% filter(dataset == &quot;dino&quot;) %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ dataset) + geom_smooth(method = &quot;lm&quot;) + coord_equal() ## `geom_smooth()` using formula &#39;y ~ x&#39; nestedDinos &lt;- dinos %&gt;% group_nest(dataset) nestedDinos %&gt;% mutate(model = map(data, ~ lm(y ~ x, data = .x )), coef = map(model, coef), resids = map(model, residuals)) ## # A tibble: 13 x 5 ## dataset data model coef resids ## &lt;chr&gt; &lt;list&lt;tbl_df[,2]&gt;&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 away [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 2 bullseye [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 3 circle [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 4 dino [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 5 dots [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 6 h_lines [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 7 high_lines [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 8 slant_down [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 9 slant_up [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 10 star [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 11 v_lines [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 12 wide_lines [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; ## 13 x_shape [142 × 2] &lt;lm&gt; &lt;dbl [2]&gt; &lt;dbl [142]&gt; dinos %&gt;% filter(dataset == &quot;dino&quot;) %&gt;% lm(data = ., y ~ x) %&gt;% broom::augment() %&gt;% ggplot(aes(x, .resid)) + geom_point() 5.4.2 Example Data Analysis As an exemplary data analysis, I quickly went through some actual lab data from y’all. Because the data is not yet published, this section only provides the code and can not be run by itself. However, I leave you with a couple of pointers to excellent learning resources to understand the vital data cleaning steps for this “untidy” dataset. This code can of course be used in an R script but I split it up into individual chunks to provide more explanation. First, we load the tidyverse and read in the raw data from and excel file # not run library(tidyverse) rawData &lt;- readxl::read_excel(&quot;data/Data Rkurs .xlsx&quot;) Not on to data tidying. The tidyr package from the tidyverse comes in handy here. The first column, which was unnamed and thus called ..1 by R. Actually contains more than one feature. So we extract the features compound and condition into their own columns with the extract function. The regular expression (regex) provides two groups with which to extract the data from the text in the original column. Check out the help page for more examples. A great resource for most data analysis tasks are the RStudio cheat sheets. They provide most of what you need in a concise format: https://rstudio.com/resources/cheatsheets/ One of those also provides a guide to the regular expressions we use in both cleaning steps: download cheatsheet. Additionally, because finding the correct regular expression can be hard, there is also an R package out there, which provides an interactive experience of building your regex called regexplain. You can find it on github: regexplain link And frankly, it is magic! tidyData &lt;- rawData %&gt;% extract(col = `...1`, into = c(&quot;compound&quot;, &quot;condition&quot;), regex = &quot;(Compound \\\\d\\\\d?) (.+)&quot;) %&gt;% pivot_longer(starts_with(&quot;E&quot;), names_to = c(&quot;experiment&quot;, &quot;replicate&quot;), names_pattern = &quot;(E\\\\d)(.?)&quot;) %&gt;% mutate(compound = fct_reorder(compound, parse_number(compound))) The data cleaning step was in fact the most complicated thing about this analysis. Once your data is in tidy format, you can use the tools we discovered in earlier weeks with ease: summaryData &lt;- tidyData %&gt;% group_by(compound, condition) %&gt;% summarise(SEM = sd(value) / sqrt(n()), value = mean(value)) signif &lt;- summaryData %&gt;% filter(compound == &quot;Compound 1&quot;) %&gt;% filter(condition == first(condition)) %&gt;% mutate(value = 100, label = &quot;*&quot;) tidyData %&gt;% ggplot(aes(compound, value, color = condition, fill = condition)) + geom_col(data = summaryData, position = position_dodge(width = 0.9), color = &quot;black&quot;) + geom_errorbar(data = summaryData, aes(ymin = value - SEM, ymax = value + SEM), position = position_dodge(width = 0.9), color = &quot;black&quot;, width = 0.5) + geom_point(position = position_dodge(width = 0.9), shape = 21, color = &quot;black&quot;) + geom_text(data = signif, aes(label = label), color = &quot;black&quot;, size = 13, position = &quot;identity&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) You might also want to run a statistical analysis, such as ANOVA, but this is just an example. Do not take this as advice for which type of analysis to run, this depends on your data and hypothesis you want to test! anovaResult &lt;- aov(value ~ compound * condition, data = tidyData) TukeyHSD(anovaResult)$condition %&gt;% head() "],
["day6.html", "Day 6 Examples and Feedback 6.1 Random Sidenotes 6.2 Lot’s of Examples", " Day 6 Examples and Feedback 6.1 Random Sidenotes This is a loose collection of data and concepts we explored. library(tidyverse) ## ── Attaching packages ──────── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.1 ## ✓ tidyr 1.1.1 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() plt &lt;- mpg %&gt;% ggplot(aes(displ, cty)) + geom_point() ggsave(filename = &quot;myPlot.png&quot;, plot = plt) ggsave uses width and height of the current graphics device by default and messages them in the console Many files, many folders? Use map! paths &lt;- dir(&quot;data&quot;, full.names = TRUE, pattern = &quot;.csv&quot;) paths &lt;- set_names(paths, basename(paths)) allDatasets &lt;- map(paths, read_csv) createDataset &lt;- function(i) { tibble( x = rnorm(100), y = rnorm(100) ) } aBunchOfDatasets &lt;- map(1:10, createDataset) names(aBunchOfDatasets) &lt;- paste0(&quot;myData/dataset &quot;, 1:10, &quot;.csv&quot;) walk2(aBunchOfDatasets, names(aBunchOfDatasets), ~ write_csv(.x, path = .y) ) paths &lt;- dir(&quot;myData&quot;, pattern = &quot;.csv&quot;, full.names = TRUE) %&gt;% set_names(basename(.)) allDatasets &lt;- map_dfr(paths, read_csv, .id = &quot;dataset&quot;) allDatasets %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~ dataset) 6.2 Lot’s of Examples 6.2.1 Tidytuesday: Spotify rawData &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## track_id = col_character(), ## track_name = col_character(), ## track_artist = col_character(), ## track_album_id = col_character(), ## track_album_name = col_character(), ## track_album_release_date = col_character(), ## playlist_name = col_character(), ## playlist_id = col_character(), ## playlist_genre = col_character(), ## playlist_subgenre = col_character() ## ) ## See spec(...) for full column specifications. spotifySongs &lt;- rawData %&gt;% mutate(track_album_release_date = lubridate::ymd(track_album_release_date)) ## Warning: Problem with `mutate()` input `track_album_release_date`. ## x 1886 failed to parse. ## ℹ Input `track_album_release_date` is `lubridate::ymd(track_album_release_date)`. ## Warning: 1886 failed to parse. Popularity with One-Hit-Wonders removed. You need to have at least 10 songs to be in my list. spotifySongs %&gt;% group_by(track_artist) %&gt;% filter(n() &gt; 10) %&gt;% summarise(popularity = mean(track_popularity)) %&gt;% arrange(desc(popularity)) %&gt;% head() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 6 x 2 ## track_artist popularity ## &lt;chr&gt; &lt;dbl&gt; ## 1 Roddy Ricch 88.2 ## 2 DaBaby 87.9 ## 3 YNW Melly 84.6 ## 4 Lewis Capaldi 83.7 ## 5 MEDUZA 83.6 ## 6 Harry Styles 83.6 getMainGenre &lt;- function(x) { names( table(x)[which.max(table(x))] ) } byArtist &lt;- spotifySongs %&gt;% group_by(track_artist) %&gt;% filter(n() &gt; 10) %&gt;% summarise(popularity = mean(track_popularity), genre = getMainGenre(playlist_genre)) %&gt;% top_n(n = 20, wt = abs(mean(popularity) - popularity)) %&gt;% mutate(track_artist = fct_reorder(track_artist, popularity)) ## `summarise()` ungrouping output (override with `.groups` argument) byArtist %&gt;% ggplot(aes(x = track_artist, y = popularity, color = genre)) + geom_linerange(aes(ymax = popularity), ymin = 0, color = &quot;grey30&quot;) + geom_point() + coord_flip() + scale_y_continuous(expand = c(0,1)) + theme_classic() + labs(x = &quot;&quot;, title = &quot;The least and most popular artists&quot;, subtitle = &quot;With at least 10 songs in the dataset&quot;) spotifySongs %&gt;% filter(playlist_genre == &quot;rock&quot;) %&gt;% ggplot(aes(track_album_release_date, valence)) + geom_point() ## Warning: Removed 804 rows containing missing values (geom_point). 6.2.2 Shiny App Create a file called app.R, copy and paste, hit run app in the top right corner. library(shiny) library(tidyverse) rawData &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv&#39;) spotifySongs &lt;- rawData %&gt;% mutate(track_album_release_date = lubridate::ymd(track_album_release_date)) ui &lt;- fluidPage( titlePanel(&quot;Old Faithful Geyser Data&quot;), sidebarLayout( sidebarPanel( selectInput(&quot;genre&quot;, &quot;Select a Genre&quot;, choices = unique(spotifySongs$playlist_genre)), selectInput(&quot;variable&quot;, &quot;Select a Genre&quot;, choices = c(&quot;valence&quot;, &quot;loudness&quot;, &quot;speechiness&quot;, &quot;liveness&quot;, &quot;energy&quot;)) ), mainPanel( plotOutput(&quot;plot&quot;) ) ) ) server &lt;- function(input, output) { output$plot &lt;- renderPlot({ spotifySongs %&gt;% filter(playlist_genre == input$genre) %&gt;% ggplot(aes(track_album_release_date, !!sym(input$variable) )) + geom_point() }) } # Run the application shinyApp(ui = ui, server = server) "],
["references.html", "References", " References Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang und Richard Iannone. 2019. rmarkdown: Dynamic Documents for R. https://CRAN.R-project.org/package=rmarkdown. O A. Ordinary Least Squares Regression Explained Visually. Explained Visually. http://setosa.io/ev/ordinary-least-squares-regression/. R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Wickham, Hadley. 2010. A Layered Grammar of Graphics. Journal of Computational and Graphical Statistics 19, Nr. 1 (Januar): 3–28. doi:10.1198/jcgs.2009.07098,. Wickham, Hadley und Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1 edition. Sebastopol, CA: O’Reilly Media. Wilkinson, Leland, D. Wills, D. Rope, A. Norton und R. Dubbs. 2005. The Grammar of Graphics. 2nd edition. New York: Springer. Xie, Yihui. 2015. Dynamic Documents with R and knitr. 2nd Aufl. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ---. 2019a. bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. ---. 2019b. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://CRAN.R-project.org/package=knitr. "]
]
